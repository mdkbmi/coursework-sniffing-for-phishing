{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"../../\"), \"src\"))\n",
    "sys.path.append(os.path.join(os.path.abspath(\".\"), \"src\"))\n",
    "os.environ['HF_HOME'] = \"/data/workspace/alexww14/2025-cv/notebooks/milstone2/cache\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/data/workspace/alexww14/2025-cv/notebooks/milstone2/cache\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from bertopic import BERTopic\n",
    "import torch\n",
    "\n",
    "from scipy.special import softmax\n",
    "from model_evaluation import get_model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/data/workspace/dataset/stacking_predictions.pkl', 'rb') as f:\n",
    "#     df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/data/workspace/dataset/sampled-dataset/raw/sample-small.parquet')\n",
    "sample_df = df.sample(100, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_prediction_label(row):\n",
    "\n",
    "#     if (row['target_1'] == 'malicious') & (row['predicted_target'] == 'malicious'):\n",
    "#         return 'true_positive'\n",
    "    \n",
    "#     if (row['target_1'] == 'malicious') & (row['predicted_target'] == 'benign'):\n",
    "#         return 'false_negative'\n",
    "    \n",
    "#     if (row['target_1'] == 'benign') & (row['predicted_target'] == 'malicious'):\n",
    "#         return 'false_positive'\n",
    "    \n",
    "#     if (row['target_1'] == 'benign') & (row['predicted_target'] == 'benign'):\n",
    "#         return 'true_negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['prediction_label'] = df.apply(lambda row: create_prediction_label(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"hello find enclose invoice payment examination access document document protect view hesitate contact question thank cal albright msw executive executive director kamloops aboriginal friendship society palm phone ext fax mail\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)\n",
    "# scores = output[0][0].detach().numpy()\n",
    "# scores = softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(model, tokenizer, text):\n",
    "    try:\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    except RuntimeError:\n",
    "        return 'str too long'\n",
    "    \n",
    "    try:\n",
    "        output = model(**encoded_input)\n",
    "    except RuntimeError:\n",
    "        return 'str too long'\n",
    "    \n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    pos = np.where(scores == max(scores))[0][0]\n",
    "\n",
    "    if pos == 0:\n",
    "        return 'negative'\n",
    "    if pos == 1:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_sentiment = sample_df[['text_preprocessed', 'target_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_sentiment['sentiment'] = toy_sentiment['text_preprocessed'].apply(lambda x: get_sentiment(model, tokenizer, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_sentiment.to_csv('/data/workspace/alexww14/2025-cv/notebooks/milestone2/BERT_sentiment_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_sentiment = pd.read_csv('/data/workspace/alexww14/2025-cv/notebooks/milestone2/BERT_sentiment_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_count = len(toy_sentiment[toy_sentiment['sentiment']=='neutral'])\n",
    "pos_count = len(toy_sentiment[toy_sentiment['sentiment']=='positive'])\n",
    "neg_count = len(toy_sentiment[toy_sentiment['sentiment']=='negative'])\n",
    "parsing_error_count = len(toy_sentiment[toy_sentiment['sentiment']=='str too long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_table = {\n",
    "    'Positive': pos_count,\n",
    "    'Neutral': neutral_count,\n",
    "    'Negative': neg_count,\n",
    "    'Parsing Error': parsing_error_count\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([sentiments_table])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic.load(\"MaartenGr/BERTopic_Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(model, text):\n",
    "    topic, prob = model.transform(text)\n",
    "\n",
    "    return model.topic_labels_[topic[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_topic = sample_df[['text_preprocessed', 'target_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_topic['topic_MaartenGr'] = toy_topic['text_preprocessed'].apply(lambda x: get_topic(topic_model, x) if x is not None else \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_topic.to_csv('/data/workspace/alexww14/2025-cv/notebooks/milestone2/BERT_topic_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count = pd.DataFrame(toy_topic[['topic_MaartenGr']].value_counts())\n",
    "topic_count.reset_index(inplace=True)\n",
    "topic_count['label_num'] = topic_count['topic_MaartenGr'].str.split(\"_\").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count.to_csv('/data/workspace/alexww14/2025-cv/notebooks/milestone2/topic_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt \n",
    "\n",
    "email_count_per_topic_chart = alt.Chart(topic_count).mark_bar().encode(\n",
    "    x=alt.X('label_num:N', sort='-y', title='Topic Label Number'),\n",
    "    y='count'\n",
    ").properties(\n",
    "    title='Email Count per Topic'\n",
    ")\n",
    "\n",
    "email_count_per_topic_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_count_per_topic_chart.save('email_count_per_topic_chart.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count = pd.DataFrame(toy_topic[['topic_MaartenGr']].value_counts())\n",
    "topic_count.reset_index(inplace=True)\n",
    "topic_count['label_num'] = topic_count['topic_MaartenGr'].str.split(\"_\").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count.to_csv('/data/workspace/alexww14/2025-cv/notebooks/milestone2/topic_count.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElSlay/BERT-Phishing-Email-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ElSlay/BERT-Phishing-Email-Model'\n",
    " \n",
    "# Load the pre-trained model and tokenizer\n",
    "model_elslay = BertForSequenceClassification.from_pretrained(model_name, cache_dir='/data/workspace/alexww14/2025-cv/notebooks/milstone2/cache')\n",
    "tokenizer_elslay = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model_elslay.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_prediction(model, tokenizer, email_text):\n",
    "\n",
    "    if not isinstance(email_text, str):\n",
    "        email_text = str(email_text)\n",
    "\n",
    "    # Tokenize and preprocess the input text\n",
    "    inputs = tokenizer(email_text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    # Make the prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Interpret the prediction\n",
    "    result = \"malicious\" if predictions.item() == 1 else \"benign\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_classification = sample_df[['text_clean', 'target_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_classification['prediction_elshay'] = toy_classification['text_clean'].apply(lambda x: BERT_prediction(model_elslay, tokenizer_elslay,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_classification['prediction_elshay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_elshay = get_model_metrics(toy_classification['target_1'], toy_classification['prediction_elshay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_elshay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/workspace/alexww14/2025-cv/results/BERT/result_elshay.pkl', 'wb') as f:\n",
    "    pickle.dump(result_elshay, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/workspace/alexww14/2025-cv/results/BERT/result_elshay.pkl\", 'rb') as f:\n",
    "    result_elshay = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([result_elshay]).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ealvaradob/bert-finetuned-phishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ealvaradob/bert-finetuned-phishing\"\n",
    " \n",
    "# Load the pre-trained model and tokenizer\n",
    "model_ealvaradob = BertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer_ealvaradob = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model_ealvaradob.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_classification['prediction_ealvaradob'] = toy_classification['text_clean'].apply(lambda x: BERT_prediction(model_ealvaradob, tokenizer_ealvaradob,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ealvaradob = get_model_metrics(toy_classification['target_1'], toy_classification['prediction_ealvaradob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_ealvaradob.pkl', 'wb') as f:\n",
    "    pickle.dump(result_ealvaradob, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_ealvaradob.pkl', 'rb') as f:\n",
    "    result_ealvaradob = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([result_ealvaradob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([result_elshay,result_ealvaradob], index=['elslay', 'ealvaradob']).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_df = pd.DataFrame({\n",
    "        'Model': results_df.index,\n",
    "        'Precision': results_df['precision'],\n",
    "        'Recall': results_df['recall'],\n",
    "        'F1-score': results_df['f1-score'],\n",
    "        'False Benign Rate / FNR': results_df['false_benign_rate'],\n",
    "        'False Malicious Rate / FPR': results_df['false_malicious_rate']\n",
    "    })\n",
    "tidy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-alexww14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
