{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c3170f",
   "metadata": {},
   "source": [
    "## Summary of work done\n",
    "\n",
    "* Used `XGBClassifier` to predict `benign` or `malicious` on `sample-large` (3000 emails)\n",
    "* 5 different iterations of `XGBClassifier`: `nontext`, `subject` (preprocessed with spacy), `body` (preprocessed with spacy) + ensemble classifiers `VotingClassifier` and `StackingClassifier` that takes in the 3 `nontext`, `subject` and `body` models\n",
    "\n",
    "Findings:\n",
    "* `subject` model has the worst performance, probably due to the lack of information that can be extracted from the subject line that is only a few words long\n",
    "* `nontext` and `body` models have similar performance &mdash; confident in predicting `malicous` but a coin toss (or worse) for `benign` (poor F1-score and horrible FPR for `benign` class)\n",
    "* `StackingClassifier` is better than `VotingClassifier`, but the performance is not too different from individual `nontext` and `body` models\n",
    "* Majority of features in `nontext` do not have large impact on SHAP values\n",
    "* Attempted hyperparameter tuning for `XGBClassifier` for `nontext` model; returned best hyperparameters values as the default values\n",
    "* Attempted feature engineering with `From_email_domain` and `Reply-To` domains but no improvement in performance\n",
    "* Attempted dropping `self_phishing` emails but no improvement in performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11961823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516e9d3",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ed903",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_parquet('/data/workspace/dataset/sampled-dataset/raw/sample-large.parquet')\n",
    "input_df = pd.read_parquet('/data/workspace/dataset/sampled-dataset/processed/sample-large.parquet')\n",
    "\n",
    "combined_df = original_df.join(input_df)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb951f",
   "metadata": {},
   "source": [
    "## Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef29ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extract_text_keywords import preprocess_text\n",
    "\n",
    "text_features = ['Subject', 'text_plain']\n",
    "numerical_features = [\n",
    "    'routing_length', 'word_count', 'readable_proportion',\n",
    "    'whitespace_ratio', 'alphabet_proportion', 'grammar_error_rate',\n",
    "    'english_french_proportion', 'url_count', \n",
    "]\n",
    "categorical_features = [\n",
    "    'dkim_result', 'spf_result', 'dmarc_result',\n",
    "    'html_parsing_error'\n",
    "]\n",
    "binary_features = [\n",
    "    'http_urls_present', 'any_long_urls', \n",
    "    'is_multipart', 'attachments_present', \n",
    "    'url_at_symbol', \n",
    "    'dmarc_authentication_present', 'dkim_sender_domains_match',\n",
    "    'to_from_addresses_match', 'sender_email_spf_match',\n",
    "    'non_ascii_present', 'hidden_text_present', #'all_urls_accessible', 'urls_redirected',\n",
    "    'ip_addr_urls',  \n",
    "    'url_port_number', 'url_multiple_subdomains',\n",
    "]\n",
    "\n",
    "input_df_columns = text_features + numerical_features + categorical_features + binary_features + ['target_1']\n",
    "input_df = combined_df[input_df_columns]\n",
    "\n",
    "input_df['Subject'] = preprocess_text(input_df['Subject'].fillna(\"\"))\n",
    "input_df['target_1'] = input_df['target_1'].map({'benign': 0, 'malicious': 1})\n",
    "input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772abc94",
   "metadata": {},
   "source": [
    "## Generate train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8603a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    input_df.drop(columns=['target_1']), input_df['target_1'],\n",
    "    train_size=0.7, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26916a86",
   "metadata": {},
   "source": [
    "## Create preprocessors and pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "preprocessor_nontext = make_column_transformer(\n",
    "    (StandardScaler(), numerical_features),\n",
    "    (OneHotEncoder(drop='if_binary', handle_unknown='ignore'), categorical_features + binary_features),\n",
    "    (\"drop\", text_features)\n",
    ")\n",
    "\n",
    "preprocessor_subject = make_column_transformer(\n",
    "    (\"drop\", numerical_features + categorical_features + binary_features + [text_features[1]]),\n",
    "    (TfidfVectorizer(), text_features[0])\n",
    ")\n",
    "\n",
    "preprocessor_body = make_column_transformer(\n",
    "    (\"drop\", numerical_features + categorical_features + binary_features + [text_features[0]]),\n",
    "    (TfidfVectorizer(), text_features[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecdd2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipe_nontext = make_pipeline(\n",
    "    preprocessor_nontext,\n",
    "    XGBClassifier(\n",
    "        n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    ")\n",
    "\n",
    "pipe_subject = make_pipeline(\n",
    "    preprocessor_subject,\n",
    "    XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    ")\n",
    "\n",
    "pipe_body = make_pipeline(\n",
    "    preprocessor_body,\n",
    "    XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78990a5",
   "metadata": {},
   "source": [
    "## Create ensemble classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb870f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimators = [('nontext', pipe_nontext), ('subject', pipe_subject), ('body', pipe_body)]\n",
    "\n",
    "vc = VotingClassifier(\n",
    "    estimators=estimators,\n",
    "    n_jobs=-1, voting='soft'\n",
    ")\n",
    "\n",
    "sc = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de89e8",
   "metadata": {},
   "source": [
    "## Cross-validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49499336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_cv_results(results):\n",
    "    return {\n",
    "        \"mean_fit_time\": results['fit_time'].mean(),\n",
    "        \"mean_score_time\": results['score_time'].mean(),\n",
    "        \"mean_train_f1\": results['train_score'].mean(),\n",
    "        \"mean_test_f1\": results['test_score'].mean(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216fe611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "models = [pipe_nontext, pipe_subject, pipe_body, vc, sc]\n",
    "names = ['pipe_nontext', 'pipe_subject', 'pipe_body', 'voting', 'stacking']\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    results = cross_validate(\n",
    "        model, X_train, y_train,\n",
    "        scoring='f1', cv=5, n_jobs=-1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    cv_results[name] = summarise_cv_results(results)\n",
    "\n",
    "pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae9381",
   "metadata": {},
   "source": [
    "## Train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc694d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "for model, name in zip(models, names):\n",
    "    y_pred = model.predict(X_train)\n",
    "    print(f'Model: {name}\\n')\n",
    "    print(classification_report(y_train, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_train, y_pred)\n",
    "    TP, TN, FP, FN = cm[1, 1], cm[0, 0], cm[0, 1], cm[1, 0]\n",
    "    FPR = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "\n",
    "    print(f'         FPR    {FPR:.05f}\\n')\n",
    "    print(cm)\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb45df4",
   "metadata": {},
   "source": [
    "## Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01c6fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, name in zip(models, names):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'Model: {name}\\n')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TP, TN, FP, FN = cm[1, 1], cm[0, 0], cm[0, 1], cm[1, 0]\n",
    "    FPR = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "\n",
    "    print(f'         FPR    {FPR:.05f}\\n')\n",
    "    print(cm)\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d84642f",
   "metadata": {},
   "source": [
    "## Ensemble classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6440fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db69f0",
   "metadata": {},
   "source": [
    "## Distribution of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = pd.DataFrame(sc.predict_proba(X_test))\n",
    "probs['true'] = y_test.tolist()\n",
    "probs['pred'] = pipe_body.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520474ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot distribution of probabilities for actual benign emails (true=0)\n",
    "benign = probs[probs['true'] == 0]\n",
    "sns.histplot(data=benign, x=0, ax=axes[0], bins=20, color='blue', alpha=0.7)\n",
    "sns.histplot(data=benign, x=1, ax=axes[0], bins=20, color='red', alpha=0.7)\n",
    "axes[0].set_title('Probability Distribution for Actual Benign Emails')\n",
    "axes[0].set_xlabel('Probability')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend(['Prob of Benign (0)', 'Prob of Malicious (1)'])\n",
    "\n",
    "# Plot distribution of probabilities for actual malicious emails (true=1)\n",
    "malicious = probs[probs['true'] == 1]\n",
    "sns.histplot(data=malicious, x=0, ax=axes[1], bins=20, color='blue', alpha=0.7)\n",
    "sns.histplot(data=malicious, x=1, ax=axes[1], bins=20, color='red', alpha=0.7)\n",
    "axes[1].set_title('Probability Distribution for Actual Malicious Emails')\n",
    "axes[1].set_xlabel('Probability')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend(['Prob of Benign (0)', 'Prob of Malicious (1)'])\n",
    "\n",
    "# Add overall title and adjust layout\n",
    "plt.suptitle('Distribution of Prediction Probabilities by True Label', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da55d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    vc, X_test, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50758c28",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(pipe_nontext['xgbclassifier'])\n",
    "observations = pd.DataFrame(\n",
    "    pipe_nontext['columntransformer'].transform(X_train),\n",
    "    columns=pipe_nontext['columntransformer'].get_feature_names_out()\n",
    ")\n",
    "shap_values = explainer.shap_values(observations)\n",
    "shap.summary_plot(shap_values, observations, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c741419",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer(observations)\n",
    "shap.plots.beeswarm(explanation, max_display=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(pipe_subject['xgbclassifier'])\n",
    "observations = pd.DataFrame(\n",
    "    pipe_subject['columntransformer'].transform(X_train).toarray(),\n",
    "    columns=pipe_subject['columntransformer'].get_feature_names_out()\n",
    ")\n",
    "shap_values = explainer.shap_values(observations)\n",
    "explanation = explainer(observations)\n",
    "shap.plots.beeswarm(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd84a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(pipe_body['xgbclassifier'])\n",
    "observations = pd.DataFrame(\n",
    "    pipe_body['columntransformer'].transform(X_train).toarray(),\n",
    "    columns=pipe_body['columntransformer'].get_feature_names_out()\n",
    ")\n",
    "shap_values = explainer.shap_values(observations)\n",
    "explanation = explainer(observations)\n",
    "shap.plots.beeswarm(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f25205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'xgbclassifier__eta': [0.01, 0.5, 0.1, 0.25, 0.5],\n",
    "    'xgbclassifier__max_depth': [3, 4, 5, 6],\n",
    "    'xgbclassifier__min_child_weight': [1, 2, 4, 8],\n",
    "    'xgbclassifier__gamma': [0.01, 0.5, 0.1, 0.25, 0.5],\n",
    "    'xgbclassifier__reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "grid_nontext = GridSearchCV(\n",
    "    pipe_nontext, param_grid, scoring=\"f1\", cv=5, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c36070",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_nontext.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_nontext.predict(X_train)\n",
    "print(f'Model: {name}\\n')\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "TP, TN, FP, FN = cm[1, 1], cm[0, 0], cm[0, 1], cm[1, 0]\n",
    "FPR = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "\n",
    "print(f'         FPR    {FPR:.05f}\\n')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e250dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_nontext.predict(X_test)\n",
    "print(f'Model: {name}\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TP, TN, FP, FN = cm[1, 1], cm[0, 0], cm[0, 1], cm[1, 0]\n",
    "FPR = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "\n",
    "print(f'         FPR    {FPR:.05f}\\n')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8268e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-danishki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
