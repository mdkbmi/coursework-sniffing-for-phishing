{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c5192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    "    FunctionTransformer,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.stats import expon, lognorm, loguniform, randint, uniform, norm\n",
    "\n",
    "# Custom feature extraction modules\n",
    "username = os.environ.get('USER')\n",
    "sys.path.append(f'/data/workspace/{username}')\n",
    "sys.path.append(os.path.join(os.path.abspath(\"../../\"), \"src\"))\n",
    "from extract_header_features import *\n",
    "from extract_text_features import *\n",
    "from extract_url_features import *\n",
    "from extract_text_keywords import *\n",
    "\n",
    "# Hide warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from DSCI571: Lecture 4 \n",
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores.iloc[i], std_scores.iloc[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset\n",
    "original_df = pd.read_parquet('/data/workspace/dataset/full-dataset/raw/train.parquet')\n",
    "# original_df = pd.read_parquet('/data/workspace/dataset/sampled-dataset/raw/sample-large.parquet')\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed50605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset\n",
    "input_df = pd.read_parquet('/data/workspace/dataset/full-dataset/processed/train.parquet')\n",
    "# input_df = pd.read_parquet('/data/workspace/dataset/sampled-dataset/processed/sample-large.parquet')\n",
    "input_df['url_count'] = get_url_count(original_df.urls)\n",
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed313fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join original_df with features_df and features_df_2\n",
    "combined_df = original_df.join(input_df)\n",
    "\n",
    "# Replace target_1 values benign to malicious when target_2 is spam\n",
    "# combined_df['target_1'] = np.where(combined_df['target_2'] == 'spam', 'malicious', combined_df['target_1'])\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out self-phishing emails\n",
    "df_without_sp = combined_df[combined_df['target_3'] != 'self_phishing'].copy()\n",
    "df_without_sp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a57f5c7",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788fb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_without_sp, test_size=0.3, random_state=42)\n",
    "\n",
    "list_cols = [\"Content_types\", \"attachment_types\", \"urls\"]\n",
    "\n",
    "for col in list_cols:\n",
    "    train_df[col] = train_df[col].apply(lambda x: \" \".join(x) if isinstance(x, (list, np.ndarray)) else str(x))\n",
    "    test_df[col] = test_df[col].apply(lambda x: \" \".join(x) if isinstance(x, (list, np.ndarray)) else str(x))\n",
    "\n",
    "\n",
    "train_df[\"Subject\"] = train_df[\"Subject\"].fillna(\"\")\n",
    "train_df[\"text_preprocessed\"] = train_df[\"text_preprocessed\"].fillna(\"\")\n",
    "\n",
    "test_df[\"Subject\"] = test_df[\"Subject\"].fillna(\"\")\n",
    "test_df[\"text_preprocessed\"] = test_df[\"text_preprocessed\"].fillna(\"\")\n",
    "\n",
    "X_train = train_df.drop(columns=['target_1'])\n",
    "y_train = train_df['target_1']\n",
    "\n",
    "X_test = test_df.drop(columns=['target_1'])\n",
    "y_test = test_df['target_1']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_num = label_encoder.fit_transform(y_train)\n",
    "y_test_num = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6204303e",
   "metadata": {},
   "source": [
    "# Preparing Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168f95f",
   "metadata": {},
   "source": [
    "## First approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = [\n",
    "    \"routing_length\", \n",
    "    \"html_parsing_error\", \n",
    "    \"word_count\", \n",
    "    \"readable_proportion\", \n",
    "    \"whitespace_ratio\", \n",
    "    \"alphabet_proportion\",\n",
    "    \"grammar_error_rate\", \n",
    "    \"english_french_proportion\",\n",
    "    \"url_count\"\n",
    "    ]\n",
    "\n",
    "binary_feats = [\n",
    "    \"is_multipart\",\n",
    "    \"dmarc_authentication_present\", \n",
    "    \"dkim_sender_domains_match\",\n",
    "    \"attachments_present\", \n",
    "    \"to_from_addresses_match\", \n",
    "    \"sender_email_spf_match\",\n",
    "    \"non_ascii_present\", \n",
    "    \"hidden_text_present\",\n",
    "    \"ip_addr_urls\",               \n",
    "    \"http_urls_present\", \n",
    "    \"url_at_symbol\",\n",
    "    \"url_port_number\", \n",
    "    \"any_long_urls\", \n",
    "    \"url_multiple_subdomains\"\n",
    "    ]\n",
    "\n",
    "text_feats = [\n",
    "    \"Content_types\" ,\n",
    "    \"urls\",\n",
    "    \"attachment_types\",\n",
    "    \"Subject\",\n",
    "    \"text_preprocessed\"\n",
    "    ]\n",
    "\n",
    "categorical_feats = [\n",
    "    \"From_name\", \n",
    "    \"From_email\", \n",
    "    \"From_email_domain\", \n",
    "    \"To_name\", \n",
    "    \"To_email\", \n",
    "    \"To_email_domain\",\n",
    "    \"dkim_result\",\n",
    "    \"spf_result\", \n",
    "    \"dmarc_result\", \n",
    "    \"Content-Language\"\n",
    "    ]\n",
    "\n",
    "drop_feats = [\n",
    "    \"From\",                         # Info extracted to From_name, From_email, From_email_domain\n",
    "    \"To\",                           # Info extracted to To_name, To_email, To_email_domain\n",
    "    \"Received\",                     # Info extracted to routing_length\n",
    "    \"Authentication-Results\",       # Info extracted to dmarc_authentication_present, dkim_result, spf_result, dmarc_result\n",
    "    \"received-spf\",                 # Info extracted to spf_result, sender_email_spf_match\n",
    "    \"DKIM-Signature\",               # Info extracted to dkim_sender_domains_match\n",
    "    \"Reply-To\",                     # Mostly missing, not useful\n",
    "    \"Return-Path\",                  # Mostly missing, not useful\n",
    "    \"text_plain\",                   \n",
    "    \"text_clean\", \n",
    "    \"text_html\", \n",
    "    \"target_2\",                     # Level 2 target variable\n",
    "    \"target_3\",                     # Level 3 target variable\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd991613",
   "metadata": {},
   "source": [
    "## Second appraoch (after feature selection):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04035b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_numeric_feats = [\n",
    "    'html_parsing_error',\n",
    "    'whitespace_ratio',\n",
    "    'grammar_error_rate',\n",
    "    'english_french_proportion'\n",
    "    ]\n",
    "    \n",
    "fs_binary_feats = [\n",
    "    'is_multipart',\n",
    "    'sender_email_spf_match'\n",
    "    ]\n",
    "\n",
    "fs_categorical_feats = [\n",
    "    'From_email',\n",
    "    'From_email_domain',\n",
    "    'To_name',\n",
    "    'To_email',\n",
    "    'To_email_domain',\n",
    "    'spf_result',\n",
    "    'Content-Language'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1239cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(StandardScaler())\n",
    "\n",
    "binary_transformer = make_pipeline(OneHotEncoder(handle_unknown='ignore', drop='if_binary'))\n",
    "\n",
    "categorical_transformer = make_pipeline(OneHotEncoder(handle_unknown='ignore', drop='if_binary'))\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_feats),\n",
    "    (binary_transformer, binary_feats),\n",
    "    (categorical_transformer, categorical_feats),\n",
    "    (CountVectorizer(), text_feats[0]), # Content_types\n",
    "    (CountVectorizer(), text_feats[1]), # urls\n",
    "    (CountVectorizer(), text_feats[2]), # attachment_types\n",
    "    (CountVectorizer(), text_feats[3]), # Subject\n",
    "    (CountVectorizer(), text_feats[4]), # Text Preprocessed\n",
    "    (\"drop\", drop_feats + text_feats + categorical_feats + binary_feats)\n",
    ")\n",
    "\n",
    "preprocessor_numeric = make_column_transformer(\n",
    "    (numeric_transformer, numeric_feats),\n",
    "    (\"drop\", drop_feats + text_feats + categorical_feats + binary_feats)\n",
    ")\n",
    "\n",
    "preprocessor_binary = make_column_transformer(\n",
    "    (binary_transformer, binary_feats),\n",
    "    (\"drop\", drop_feats + text_feats + categorical_feats + numeric_feats)\n",
    ")\n",
    "\n",
    "preprocessor_categorical = make_column_transformer(\n",
    "    (categorical_transformer, categorical_feats),\n",
    "    (\"drop\", drop_feats + text_feats + numeric_feats + binary_feats)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99894df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, fs_numeric_feats),\n",
    "    (binary_transformer, fs_binary_feats),\n",
    "    (categorical_transformer, fs_categorical_feats),\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55144045",
   "metadata": {},
   "source": [
    "## Third approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793fc5e",
   "metadata": {},
   "source": [
    "### Header Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_numeric_feats = [\n",
    "    \"routing_length\"\n",
    "]\n",
    "\n",
    "header_binary_feats = [\n",
    "    \"is_multipart\",\n",
    "    \"dmarc_authentication_present\", \n",
    "    \"dkim_sender_domains_match\",\n",
    "    \"attachments_present\", \n",
    "    \"to_from_addresses_match\", \n",
    "    \"sender_email_spf_match\"\n",
    "]\n",
    "\n",
    "header_categorical_feats = [\n",
    "    \"From_name\", \n",
    "    \"From_email\", \n",
    "    \"From_email_domain\", \n",
    "    \"To_name\", \n",
    "    \"To_email\", \n",
    "    \"To_email_domain\",\n",
    "    \"dkim_result\",\n",
    "    \"spf_result\", \n",
    "    \"dmarc_result\", \n",
    "    \"Content-Language\"\n",
    "]\n",
    "\n",
    "header_text_feats = [\n",
    "    \"Subject\"\n",
    "]\n",
    "\n",
    "header_drop_feats = [\n",
    "    \"From\",                         # Info extracted to From_name, From_email, From_email_domain\n",
    "    \"To\",                           # Info extracted to To_name, To_email, To_email_domain\n",
    "    \"Received\",                     # Info extracted to routing_length\n",
    "    \"Authentication-Results\",       # Info extracted to dmarc_authentication_present, dkim_result, spf_result, dmarc_result\n",
    "    \"received-spf\",                 # Info extracted to spf_result, sender_email_spf_match\n",
    "    \"DKIM-Signature\",               # Info extracted to dkim_sender_domains_match\n",
    "    \"Reply-To\",                     # Mostly missing, not useful\n",
    "    \"Return-Path\",                  # Mostly missing, not useful\n",
    "    \"text_plain\",                   \n",
    "    \"text_clean\", \n",
    "    \"text_html\"\n",
    "]\n",
    "\n",
    "subject_vectorizer = make_pipeline(CountVectorizer())\n",
    "\n",
    "preprocessor_header = make_column_transformer(\n",
    "    (\"passthrough\", header_numeric_feats),\n",
    "    (binary_transformer, header_binary_feats),\n",
    "    (categorical_transformer, header_categorical_feats),\n",
    "    (subject_vectorizer, header_text_feats[0]), # Subject\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d9d9ec",
   "metadata": {},
   "source": [
    "### Header without Subject preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_header_without_subject = make_column_transformer(\n",
    "    (\"passthrough\", header_numeric_feats),\n",
    "    (binary_transformer, header_binary_feats),\n",
    "    (categorical_transformer, header_categorical_feats),\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da629504",
   "metadata": {},
   "source": [
    "### Body Prepocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3087af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_numeric_feats = [\n",
    "        \"word_count\",\n",
    "        \"readable_proportion\",\n",
    "        \"whitespace_ratio\",\n",
    "        \"alphabet_proportion\",\n",
    "        \"grammar_error_rate\",\n",
    "        \"english_french_proportion\",\n",
    "        \"url_count\"\n",
    "]\n",
    "\n",
    "body_binary_feats = [\n",
    "        \"non_ascii_present\",\n",
    "        \"hidden_text_present\",\n",
    "        \"all_urls_accessible\",\n",
    "        \"urls_redirected\",\n",
    "        \"ip_addr_urls\",\n",
    "        \"http_urls_present\",\n",
    "        \"url_at_symbol\",\n",
    "        \"url_port_number\",\n",
    "        \"any_long_urls\",\n",
    "        \"url_multiple_subdomains\"\n",
    "]\n",
    "\n",
    "body_categorical_feats = [\n",
    "        \"html_parsing_error\"\n",
    "]\n",
    "\n",
    "body_text_feats = [\n",
    "        \"Content_types\",\n",
    "        \"attachment_types\",\n",
    "        \"text_preprocessed\",\n",
    "        \"urls\"\n",
    "]\n",
    "\n",
    "content_types_vectorizer = make_pipeline(CountVectorizer())\n",
    "attachment_types_vectorizer = make_pipeline(CountVectorizer())\n",
    "text_preprocessed_vectorizer = make_pipeline(CountVectorizer())\n",
    "urls_vectorizer = make_pipeline(CountVectorizer())\n",
    "\n",
    "\n",
    "preprocessor_body = make_column_transformer(\n",
    "        (numeric_transformer, body_numeric_feats),\n",
    "        (binary_transformer, body_binary_feats),\n",
    "        (categorical_transformer, body_categorical_feats),\n",
    "        (content_types_vectorizer, body_text_feats[0]), # content_types\n",
    "        (attachment_types_vectorizer, body_text_feats[1]), # attachment_types\n",
    "        (text_preprocessed_vectorizer, body_text_feats[2]), # text_preprocessed\n",
    "        (urls_vectorizer, body_text_feats[3]), # urls\n",
    "        remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e8960",
   "metadata": {},
   "source": [
    "### Body without text_preprocessed preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_body_without_text_pp = make_column_transformer(\n",
    "        (numeric_transformer, body_numeric_feats),\n",
    "        (binary_transformer, body_binary_feats),\n",
    "        (categorical_transformer, body_categorical_feats),\n",
    "        (CountVectorizer(), body_text_feats[0]), # content_types\n",
    "        (CountVectorizer(), body_text_feats[1]), # attachment_types\n",
    "        (CountVectorizer(), body_text_feats[3]), # urls\n",
    "        remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43128426",
   "metadata": {},
   "source": [
    "### Text Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_text = make_column_transformer(\n",
    "    (CountVectorizer(), text_feats[3]), # Subject\n",
    "    (CountVectorizer(), text_feats[4]), # Text Preprocessed\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba8c5e",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f3837",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "X_train_smote, y_train_smote = SMOTE().fit_resample(X_train_transformed, y_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cbb73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs_transformed = fs_preprocessor.fit_transform(X_train)\n",
    "X_test_fs_transformed = fs_preprocessor.transform(X_test)\n",
    "\n",
    "X_train_fs_smote, y_train_fs_smote = SMOTE().fit_resample(X_train_fs_transformed, y_train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6ca56",
   "metadata": {},
   "source": [
    "## ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_adasyn, y_train_adasyn = ADASYN().fit_resample(X_train_transformed, y_train_num)\n",
    "X_train_fs_adasyn, y_train_fs_adasyn = ADASYN().fit_resample(X_train_fs_transformed, y_train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d4436",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9641b6f2",
   "metadata": {},
   "source": [
    "## Dictionaries setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef476d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train evaluation dictionaries\n",
    "trained_models = {}\n",
    "train_predictions = {}\n",
    "train_classification_report_dict = {}\n",
    "train_confusion_matrices = {}\n",
    "train_fpr_dict = {}\n",
    "train_f1_benign_dict = {}\n",
    "train_f1_malicious_dict = {}\n",
    "\n",
    "# Validation evaluation dictionaries\n",
    "test_predictions = {}\n",
    "test_classification_report_dict = {}\n",
    "test_confusion_matrices = {}\n",
    "test_fpr_dict = {}\n",
    "test_f1_benign_dict = {}\n",
    "test_f1_malicious_dict = {}\n",
    "\n",
    "# CV results\n",
    "scoring = ['accuracy', 'f1', 'precision', 'recall']\n",
    "results_df = None\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68648826",
   "metadata": {},
   "source": [
    "**Evaluation function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe3965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_store_results(model_name, model, X_train, y_train, X_test, y_test, label_encoder):\n",
    "    y_pred = model.predict(X_train)\n",
    "    trained_models[model_name] = model\n",
    "    train_predictions[model_name] = y_pred\n",
    "\n",
    "    # Training evaluation\n",
    "    train_classification_report_dict[model_name] = classification_report(\n",
    "        y_train, y_pred, target_names=label_encoder.classes_, output_dict=True\n",
    "    )\n",
    "    train_f1_benign_dict[model_name] = f1_score(y_train, y_pred, pos_label=0)\n",
    "    train_f1_malicious_dict[model_name] = f1_score(y_train, y_pred, pos_label=1)\n",
    "    cm = confusion_matrix(y_train, y_pred)\n",
    "    train_confusion_matrices[model_name] = cm\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    train_fpr_dict[model_name] = FP / (FP + TN)\n",
    "\n",
    "    # Test evaluation\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_predictions[model_name] = y_test_pred\n",
    "    test_classification_report_dict[model_name] = classification_report(\n",
    "        y_test, y_test_pred, target_names=label_encoder.classes_, output_dict=True\n",
    "    )\n",
    "    test_f1_benign_dict[model_name] = f1_score(y_test, y_test_pred, pos_label=0)\n",
    "    test_f1_malicious_dict[model_name] = f1_score(y_test, y_test_pred, pos_label=1)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "    test_confusion_matrices[model_name] = cm_test\n",
    "    TN, FP, FN, TP = cm_test.ravel()\n",
    "    test_fpr_dict[model_name] = FP / (FP + TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d2fdb",
   "metadata": {},
   "source": [
    "## 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d9748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DecisionTreeClassifier\"\n",
    "\n",
    "model = DecisionTreeClassifier(class_weight='balanced', random_state=123)\n",
    "model.fit(X_train_transformed, y_train_num)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_transformed, y_train_num, X_test_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f17d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DecisionTreeClassifier_fs\"\n",
    "\n",
    "model = DecisionTreeClassifier(class_weight='balanced', random_state=123)\n",
    "model.fit(X_train_fs_transformed, y_train_num)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_fs_transformed, y_train_num, X_test_fs_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9de0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DecisionTreeClassifier_SMOTE\"\n",
    "\n",
    "model = DecisionTreeClassifier(class_weight='balanced', random_state=123)\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_smote, y_train_smote, X_test_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29caf29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DecisionTreeClassifier_SMOTE_fs\"\n",
    "\n",
    "model = DecisionTreeClassifier(class_weight='balanced', random_state=123)\n",
    "model.fit(X_train_fs_smote, y_train_fs_smote)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_fs_smote, y_train_fs_smote, X_test_fs_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26435d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DecisionTreeClassifier_ADASYN\"\n",
    "\n",
    "model = DecisionTreeClassifier(class_weight='balanced', random_state=123)\n",
    "model.fit(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_adasyn, y_train_adasyn, X_test_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DecisionTreeClassifier_ADASYN_fs\"\n",
    "\n",
    "model = DecisionTreeClassifier(class_weight='balanced', random_state=123)\n",
    "model.fit(X_train_fs_adasyn, y_train_fs_adasyn)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_fs_adasyn, y_train_fs_adasyn, X_test_fs_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2c350",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bacc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RandomForestClassifier\"\n",
    "\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=123, n_jobs=-1)\n",
    "model.fit(X_train_transformed, y_train_num)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_transformed, y_train_num, X_test_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bccb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RandomForestClassifier_fs\"\n",
    "\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=123, n_jobs=-1)\n",
    "model.fit(X_train_fs_transformed, y_train_num)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_fs_transformed, y_train_num, X_test_fs_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2456aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RandomForestClassifier_SMOTE\"\n",
    "\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=123, n_jobs=-1)\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_smote, y_train_smote, X_test_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a00ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RandomForestClassifier_SMOTE_fs\"\n",
    "\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=123, n_jobs=-1)\n",
    "model.fit(X_train_fs_smote, y_train_fs_smote)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_fs_smote, y_train_fs_smote, X_test_fs_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80612183",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RandomForestClassifier_ADASYN\"\n",
    "\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=123, n_jobs=-1)\n",
    "model.fit(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_adasyn, y_train_adasyn, X_test_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212abd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RandomForestClassifier_ADASYN_fs\"\n",
    "\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=123, n_jobs=-1)\n",
    "model.fit(X_train_fs_adasyn, y_train_fs_adasyn)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_fs_adasyn, y_train_fs_adasyn, X_test_fs_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c547ba",
   "metadata": {},
   "source": [
    "## 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "classes_weights = class_weight.compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=train_df['target_1']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"XGBClassifier\"\n",
    "\n",
    "model = XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    "model.fit(X_train_transformed, y_train_num, sample_weight=classes_weights)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_transformed, y_train_num, X_test_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"XGBClassifier_fs\"\n",
    "\n",
    "model = XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    "model.fit(X_train_fs_transformed, y_train_num, sample_weight=classes_weights)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_fs_transformed, y_train_num, X_test_fs_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52013209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"XGBClassifier_SMOTE\"\n",
    "\n",
    "model = XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_smote, y_train_smote, X_test_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66286530",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"XGBClassifier_SMOTE_fs\"\n",
    "\n",
    "model = XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    "model.fit(X_train_fs_smote, y_train_fs_smote)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_fs_smote, y_train_fs_smote, X_test_fs_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27928ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"XGBClassifier_ADASYN\"\n",
    "\n",
    "model = XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    "model.fit(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_adasyn, y_train_adasyn, X_test_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"XGBClassifier_ADASYN_fs\"\n",
    "\n",
    "model = XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    "model.fit(X_train_fs_adasyn, y_train_fs_adasyn)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train_fs_adasyn, y_train_fs_adasyn, X_test_fs_transformed, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a09d5",
   "metadata": {},
   "source": [
    "## 4. XGBoost (Header only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201bd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"XGBClassifier_header\"\n",
    "\n",
    "xgb = XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    "model = make_pipeline(preprocessor_header, xgb)\n",
    "model.fit(X_train, y_train_num, xgbclassifier__sample_weight=classes_weights)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train, y_train_num, X_test, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910aaea8",
   "metadata": {},
   "source": [
    "## 5. XGBoost (Body only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"XGBClassifier_body\"\n",
    "\n",
    "xgb = XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    "model = make_pipeline(preprocessor_body, xgb)\n",
    "model.fit(X_train, y_train_num, xgbclassifier__sample_weight=classes_weights)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train, y_train_num, X_test, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36865b0d",
   "metadata": {},
   "source": [
    "## 5. Stacking (XGBoost: header + body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_header = make_pipeline(\n",
    "    preprocessor_header,\n",
    "    XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    ")\n",
    "\n",
    "pipe_body = make_pipeline(\n",
    "    preprocessor_body,\n",
    "    XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = [(\"header\", pipe_header), (\"body\", pipe_body)]\n",
    "\n",
    "stacking = StackingClassifier(\n",
    "    estimators=estimator,\n",
    "    final_estimator=SVC(kernel='rbf', class_weight='balanced', random_state=123),\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e44dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['Stacking (XGB_header + XGB_body)'] = mean_std_cross_val_scores(stacking, X_train, y_train_num, cv=5, scoring=scoring, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfec188",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(results_dict).T\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Stacking (XGB_header + XGB_body)\"\n",
    "\n",
    "model = stacking\n",
    "model.fit(X_train, y_train_num)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train, y_train_num, X_test, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c174ca",
   "metadata": {},
   "source": [
    "## 6. Voting (XGBoost: header + body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = VotingClassifier(\n",
    "    estimators=estimator,\n",
    "    voting='soft',\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Voting (lr_header + lr_body + XGB_header + XGB_body)\"\n",
    "\n",
    "model = voting\n",
    "model.fit(X_train, y_train_num)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train, y_train_num, X_test, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c506f",
   "metadata": {},
   "source": [
    "## 7. XGBoost (Text only: subject + text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62337df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"XGBClassifier_text\"\n",
    "\n",
    "xgb = XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    "model = make_pipeline(preprocessor_text, xgb)\n",
    "model.fit(X_train, y_train_num, xgbclassifier__sample_weight=classes_weights)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train, y_train_num, X_test, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a275180b",
   "metadata": {},
   "source": [
    "## 8. Stacking (3 different XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ecfcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_header_without_subject = make_pipeline(\n",
    "    preprocessor_header_without_subject,\n",
    "    XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    ")\n",
    "\n",
    "pipe_body_without_text_preprocessed = make_pipeline(\n",
    "    preprocessor_body_without_text_pp,\n",
    "    XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    ")\n",
    "\n",
    "pipe_subject_text_pp = make_pipeline(\n",
    "    preprocessor_text,\n",
    "    XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = [(\"header_without_subject\", pipe_header_without_subject), (\"body_without_text_pp\", pipe_body_without_text_preprocessed), (\"subject_text_pp\", pipe_subject_text_pp)]\n",
    "\n",
    "stacking_v2 = StackingClassifier(\n",
    "    estimators=estimator,\n",
    "    final_estimator=SVC(kernel='rbf', class_weight='balanced', random_state=123),\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Stacking (XGB_header_x_subject + XGB_body_x_text_pp + XGB_subject_text_pp)\"\n",
    "\n",
    "model = stacking_v2\n",
    "model.fit(X_train, y_train_num)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train, y_train_num, X_test, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab4cd3",
   "metadata": {},
   "source": [
    "## Tuned Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8058f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best parameters\n",
    "stacking_best_param = pickle.load(open(\"ht_script_result.pkl\", \"rb\"))\n",
    "stacking_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a6e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_header = make_pipeline(\n",
    "    preprocessor_header,\n",
    "    XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    ")\n",
    "\n",
    "pipe_body = make_pipeline(\n",
    "    preprocessor_body,\n",
    "    XGBClassifier(n_jobs=-1, eval_metric=\"error\", objective=\"binary:logistic\")\n",
    ")\n",
    "estimator = [(\"header\", pipe_header), (\"body\", pipe_body)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e07e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_tuned = StackingClassifier(\n",
    "    estimators=estimator,\n",
    "    final_estimator=SVC(kernel='rbf', class_weight='balanced', random_state=123),\n",
    "    n_jobs=-1\n",
    ").set_params(**stacking_best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00695779",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Stacking_tuned (XGB_header + XGB_body)\"\n",
    "\n",
    "model = stacking_tuned\n",
    "model.fit(X_train, y_train_num)\n",
    "\n",
    "evaluate_and_store_results(model_name, model, X_train, y_train_num, X_test, y_test_num, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca11fd0b",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "**Note:** This part's work has been commented out as we have found that the model performed better without feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9116977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b24b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"feature_selection_decision_tree\"\n",
    "\n",
    "# pipe_lgr_dt = make_pipeline(\n",
    "#     preprocessor_numeric,\n",
    "#     SelectFromModel(LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=0.01)),\n",
    "#     DecisionTreeClassifier(class_weight='balanced', random_state=123),\n",
    "# )\n",
    "# pipe_lgr_dt.fit(X_train, y_train_num)\n",
    "\n",
    "# y_pred = pipe_lgr_dt.predict(X_train)\n",
    "    \n",
    "# trained_models[model_name] = pipe_lgr_dt\n",
    "# train_predictions[model_name] = y_pred\n",
    "\n",
    "\n",
    "# # Training evaluation\n",
    "# # Classification report\n",
    "# train_classification_report_dict[model_name] = classification_report(\n",
    "#     y_train_num, y_pred, target_names=label_encoder.classes_\n",
    "# )\n",
    "\n",
    "# # F1 scores for each class\n",
    "# train_f1_benign_dict[model_name] = f1_score(y_train_num, y_pred, pos_label=0)\n",
    "# train_f1_malicious_dict[model_name] =  f1_score(y_train_num, y_pred, pos_label=1)\n",
    "\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(y_train_num, y_pred)\n",
    "# train_confusion_matrices[model_name] = cm\n",
    "\n",
    "# # False Positive Rate\n",
    "# TN, FP, FN, TP = cm.ravel()\n",
    "# fpr = FP / (FP + TN)\n",
    "# train_fpr_dict[model_name] = fpr\n",
    "\n",
    "# # Test evaluation\n",
    "# y_test_pred = pipe_lgr_dt.predict(X_test)\n",
    "# test_predictions[model_name] = y_test_pred\n",
    "\n",
    "# test_classification_report_dict[model_name] = classification_report(\n",
    "#     y_test_num, y_test_pred, target_names=label_encoder.classes_\n",
    "# )\n",
    "# test_f1_benign_dict[model_name] = f1_score(y_test_num, y_test_pred, pos_label=0)\n",
    "# test_f1_malicious_dict[model_name] = f1_score(y_test_num, y_test_pred, pos_label=1)\n",
    "\n",
    "# cm_test = confusion_matrix(y_test_num, y_test_pred)\n",
    "# test_confusion_matrices[model_name] = cm_test\n",
    "# TN, FP, FN, TP = cm_test.ravel()\n",
    "# test_fpr_dict[model_name] = FP / (FP + TN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1_coefs = pipe_lgr_dt.named_steps[\"selectfrommodel\"].estimator_.coef_.flatten()\n",
    "# fs_num = pd.DataFrame(l1_coefs, index=numeric_feats, columns=[\"l1_coefs\"])\n",
    "\n",
    "# useful_feats = fs_num[fs_num['l1_coefs'] != 0].index.tolist()\n",
    "\n",
    "# fs_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe193362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"feature_selection_decision_tree\"\n",
    "\n",
    "# pipe_lgr_dt = make_pipeline(\n",
    "#     preprocessor_binary,\n",
    "#     SelectFromModel(LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=0.01)),\n",
    "#     DecisionTreeClassifier(class_weight='balanced', random_state=123),\n",
    "# )\n",
    "# pipe_lgr_dt.fit(X_train, y_train_num)\n",
    "\n",
    "# y_pred = pipe_lgr_dt.predict(X_train)\n",
    "    \n",
    "# trained_models[model_name] = pipe_lgr_dt\n",
    "# train_predictions[model_name] = y_pred\n",
    "\n",
    "\n",
    "# # Training evaluation\n",
    "# # Classification report\n",
    "# train_classification_report_dict[model_name] = classification_report(\n",
    "#     y_train_num, y_pred, target_names=label_encoder.classes_\n",
    "# )\n",
    "\n",
    "# # F1 scores for each class\n",
    "# train_f1_benign_dict[model_name] = f1_score(y_train_num, y_pred, pos_label=0)\n",
    "# train_f1_malicious_dict[model_name] =  f1_score(y_train_num, y_pred, pos_label=1)\n",
    "\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(y_train_num, y_pred)\n",
    "# train_confusion_matrices[model_name] = cm\n",
    "\n",
    "# # False Positive Rate\n",
    "# TN, FP, FN, TP = cm.ravel()\n",
    "# fpr = FP / (FP + TN)\n",
    "# train_fpr_dict[model_name] = fpr\n",
    "\n",
    "# # Test evaluation\n",
    "# y_test_pred = pipe_lgr_dt.predict(X_test)\n",
    "# test_predictions[model_name] = y_test_pred\n",
    "\n",
    "# test_classification_report_dict[model_name] = classification_report(\n",
    "#     y_test_num, y_test_pred, target_names=label_encoder.classes_\n",
    "# )\n",
    "# test_f1_benign_dict[model_name] = f1_score(y_test_num, y_test_pred, pos_label=0)\n",
    "# test_f1_malicious_dict[model_name] = f1_score(y_test_num, y_test_pred, pos_label=1)\n",
    "\n",
    "# cm_test = confusion_matrix(y_test_num, y_test_pred)\n",
    "# test_confusion_matrices[model_name] = cm_test\n",
    "# TN, FP, FN, TP = cm_test.ravel()\n",
    "# test_fpr_dict[model_name] = FP / (FP + TN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1_coefs = pipe_lgr_dt.named_steps[\"selectfrommodel\"].estimator_.coef_.flatten()\n",
    "# fs_bin = pd.DataFrame(l1_coefs, index=binary_feats, columns=[\"l1_coefs\"])\n",
    "\n",
    "# # Add the useful features to the list\n",
    "# # useful_feats += fs_bin[fs_bin['l1_coefs'] != 0].index.tolist()\n",
    "\n",
    "# fs_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a920b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"feature_selection_decision_tree\"\n",
    "\n",
    "# pipe_lgr_dt = make_pipeline(\n",
    "#     preprocessor_categorical,\n",
    "#     SelectFromModel(LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=0.01)),\n",
    "#     DecisionTreeClassifier(class_weight='balanced', random_state=123),\n",
    "# )\n",
    "# pipe_lgr_dt.fit(X_train, y_train_num)\n",
    "\n",
    "# y_pred = pipe_lgr_dt.predict(X_train)\n",
    "    \n",
    "# trained_models[model_name] = pipe_lgr_dt\n",
    "# train_predictions[model_name] = y_pred\n",
    "\n",
    "\n",
    "# # Training evaluation\n",
    "# # Classification report\n",
    "# train_classification_report_dict[model_name] = classification_report(\n",
    "#     y_train_num, y_pred, target_names=label_encoder.classes_\n",
    "# )\n",
    "\n",
    "# # F1 scores for each class\n",
    "# train_f1_benign_dict[model_name] = f1_score(y_train_num, y_pred, pos_label=0)\n",
    "# train_f1_malicious_dict[model_name] =  f1_score(y_train_num, y_pred, pos_label=1)\n",
    "\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(y_train_num, y_pred)\n",
    "# train_confusion_matrices[model_name] = cm\n",
    "\n",
    "# # False Positive Rate\n",
    "# TN, FP, FN, TP = cm.ravel()\n",
    "# fpr = FP / (FP + TN)\n",
    "# train_fpr_dict[model_name] = fpr\n",
    "\n",
    "# # Test evaluation\n",
    "# y_test_pred = pipe_lgr_dt.predict(X_test)\n",
    "# test_predictions[model_name] = y_test_pred\n",
    "\n",
    "# test_classification_report_dict[model_name] = classification_report(\n",
    "#     y_test_num, y_test_pred, target_names=label_encoder.classes_\n",
    "# )\n",
    "# test_f1_benign_dict[model_name] = f1_score(y_test_num, y_test_pred, pos_label=0)\n",
    "# test_f1_malicious_dict[model_name] = f1_score(y_test_num, y_test_pred, pos_label=1)\n",
    "\n",
    "# cm_test = confusion_matrix(y_test_num, y_test_pred)\n",
    "# test_confusion_matrices[model_name] = cm_test\n",
    "# TN, FP, FN, TP = cm_test.ravel()\n",
    "# test_fpr_dict[model_name] = FP / (FP + TN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a90da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe = pipe_lgr_dt.named_steps[\"columntransformer\"].named_transformers_['pipeline'].named_steps['onehotencoder']\n",
    "\n",
    "# # Get the encoded feature names\n",
    "# feature_names = ohe.get_feature_names_out(categorical_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0279b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1_coefs = pipe_lgr_dt.named_steps[\"selectfrommodel\"].estimator_.coef_.flatten()\n",
    "# fs_cat = pd.DataFrame(l1_coefs, index=feature_names, columns=[\"l1_coefs\"])\n",
    "\n",
    "# # Add the useful features to the list\n",
    "# # useful_feats += fs_cat[fs_cat['l1_coefs'] != 0].index.tolist()\n",
    "\n",
    "# # Filter out the features with non-zero coefficients\n",
    "# fs_cat = fs_cat[fs_cat['l1_coefs'] != 0].index.tolist()\n",
    "# fs_cat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7138c7e0",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca72d30",
   "metadata": {},
   "source": [
    "## Train evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d445246",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_df = pd.DataFrame({\n",
    "    \"Model\": list(trained_models.keys()),\n",
    "    \"Precision Benign\": [train_classification_report_dict[model_name][\"benign\"][\"precision\"] for model_name in trained_models.keys()],\n",
    "    \"Precision Malicious\": [train_classification_report_dict[model_name][\"malicious\"][\"precision\"] for model_name in trained_models.keys()],\n",
    "    \"Recall Benign\": [train_classification_report_dict[model_name][\"benign\"][\"recall\"] for model_name in trained_models.keys()],\n",
    "    \"Recall Malicious\": [train_classification_report_dict[model_name][\"malicious\"][\"recall\"] for model_name in trained_models.keys()],\n",
    "    \"F1 Benign\": list(train_f1_benign_dict.values()),\n",
    "    \"F1 Malicious\": list(train_f1_malicious_dict.values()),\n",
    "    \"FPR\": list(train_fpr_dict.values()),\n",
    "    \"confusion_matrix\": list(train_confusion_matrices.values()),\n",
    "})\n",
    "\n",
    "float_cols = train_results_df.select_dtypes(include='float').columns\n",
    "train_results_df[float_cols] = train_results_df[float_cols].round(2)\n",
    "train_results_df = train_results_df.set_index(\"Model\")\n",
    "train_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f7b870",
   "metadata": {},
   "source": [
    "## Validation evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf34a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame({\n",
    "    \"Model\": list(trained_models.keys()),\n",
    "    \"Precision Benign\": [test_classification_report_dict[model_name]['benign']['precision'] for model_name in trained_models.keys()],\n",
    "    \"Precision Malicious\": [test_classification_report_dict[model_name]['malicious']['precision'] for model_name in trained_models.keys()],\n",
    "    \"Recall Benign\": [test_classification_report_dict[model_name]['benign']['recall'] for model_name in trained_models.keys()],\n",
    "    \"Recall Malicious\": [test_classification_report_dict[model_name]['malicious']['recall'] for model_name in trained_models.keys()],\n",
    "    \"F1 Benign\": list(test_f1_benign_dict.values()),\n",
    "    \"F1 Malicious\": list(test_f1_malicious_dict.values()),\n",
    "    \"FPR\": list(test_fpr_dict.values()),\n",
    "    \"Confusion Matrix\": list(test_confusion_matrices.values())\n",
    "})\n",
    "\n",
    "float_cols = test_results_df.select_dtypes(include='float').columns\n",
    "test_results_df[float_cols] = test_results_df[float_cols].round(2)\n",
    "test_results_df = test_results_df.set_index(\"Model\")\n",
    "test_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b05eb9",
   "metadata": {},
   "source": [
    "# Understanding the best model's prediction\n",
    "\n",
    "The stacking model with two xgb model seems to perform the best. Now let's take a look at it's predictions and investigate what aspect of it is doing poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bc720",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_prediction = test_predictions['Stacking (XGB_header + XGB_body)']\n",
    "\n",
    "test_df_with_predictions = test_df.copy()\n",
    "\n",
    "# Move the target_1 column to the last position\n",
    "target_col = test_df_with_predictions.pop('target_1')\n",
    "test_df_with_predictions['target_1'] = target_col\n",
    "test_df_with_predictions['predicted_target'] = stacking_prediction\n",
    "test_df_with_predictions['predicted_target'] = test_df_with_predictions['predicted_target'].replace({0: 'benign', 1: 'malicious'})\n",
    "test_df_with_predictions['correct_prediction'] = test_df_with_predictions['predicted_target'] == test_df_with_predictions['target_1']\n",
    "test_df_with_predictions['True Positive'] = (test_df_with_predictions['predicted_target'] == 'malicious') & (test_df_with_predictions['target_1'] == 'malicious')\n",
    "test_df_with_predictions['True Negative'] = (test_df_with_predictions['predicted_target'] == 'benign') & (test_df_with_predictions['target_1'] == 'benign')\n",
    "test_df_with_predictions['False Positive'] = (test_df_with_predictions['predicted_target'] == 'malicious') & (test_df_with_predictions['target_1'] == 'benign')\n",
    "test_df_with_predictions['False Negative'] = (test_df_with_predictions['predicted_target'] == 'benign') & (test_df_with_predictions['target_1'] == 'malicious')\n",
    "# test_df_with_predictions.reset_index(inplace=True)\n",
    "test_df_with_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the DataFrame as a pickle file\n",
    "# test_df_with_predictions.to_pickle(\"/data/workspace/jiaquan/stacking_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6167f52f",
   "metadata": {},
   "source": [
    "## Analyse predicted labels mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the pickle dataframe\n",
    "prediction_df = pd.read_pickle(\"/data/workspace/dataset/stacking_predictions.pkl\")\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6164a",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fitted header pipeline from stacking\n",
    "header_pipe = trained_models['Stacking (XGB_header + XGB_body)'].named_estimators_['header']\n",
    "\n",
    "# Get the XGBoost model\n",
    "xgb_model = header_pipe.named_steps['xgbclassifier']\n",
    "\n",
    "# Get feature importances\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "# Get feature names from the preprocessor\n",
    "def get_feature_names_from_column_transformer(column_transformer):\n",
    "    feature_names = []\n",
    "    for name, transformer, cols in column_transformer.transformers_:\n",
    "        if name == \"remainder\" and transformer == \"drop\":\n",
    "            continue\n",
    "        if hasattr(transformer, 'get_feature_names_out'):\n",
    "            names = transformer.get_feature_names_out(cols)\n",
    "        elif hasattr(transformer, 'named_steps'):\n",
    "            # For pipelines\n",
    "            last_step = list(transformer.named_steps.values())[-1]\n",
    "            if hasattr(last_step, 'get_feature_names_out'):\n",
    "                names = last_step.get_feature_names_out(cols)\n",
    "            else:\n",
    "                names = cols\n",
    "        else:\n",
    "            names = cols\n",
    "        feature_names.extend(names)\n",
    "    return feature_names\n",
    "\n",
    "# Use the fitted column transformer from the pipeline\n",
    "fitted_column_transformer = header_pipe.named_steps['columntransformer']\n",
    "feature_names = get_feature_names_from_column_transformer(fitted_column_transformer)\n",
    "\n",
    "# Combine into a DataFrame for easy viewing\n",
    "header_feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "header_feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fitted header pipeline from stacking\n",
    "body_pipe = trained_models['Stacking (XGB_header + XGB_body)'].named_estimators_['body']\n",
    "\n",
    "# Get the XGBoost model\n",
    "xgb_model = body_pipe.named_steps['xgbclassifier']\n",
    "\n",
    "# Get feature importances\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "# Get feature names from the preprocessor\n",
    "def get_feature_names_from_column_transformer(column_transformer):\n",
    "    feature_names = []\n",
    "    for name, transformer, cols in column_transformer.transformers_:\n",
    "        if name == \"remainder\" and transformer == \"drop\":\n",
    "            continue\n",
    "        if hasattr(transformer, 'get_feature_names_out'):\n",
    "            names = transformer.get_feature_names_out(cols)\n",
    "        elif hasattr(transformer, 'named_steps'):\n",
    "            # For pipelines\n",
    "            last_step = list(transformer.named_steps.values())[-1]\n",
    "            if hasattr(last_step, 'get_feature_names_out'):\n",
    "                names = last_step.get_feature_names_out(cols)\n",
    "            else:\n",
    "                names = cols\n",
    "        else:\n",
    "            names = cols\n",
    "        feature_names.extend(names)\n",
    "    return feature_names\n",
    "\n",
    "# Use the fitted column transformer from the pipeline\n",
    "fitted_column_transformer = body_pipe.named_steps['columntransformer']\n",
    "feature_names = get_feature_names_from_column_transformer(fitted_column_transformer)\n",
    "\n",
    "# Combine into a DataFrame for easy viewing\n",
    "body_feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "body_feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec9af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-jiaquan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
