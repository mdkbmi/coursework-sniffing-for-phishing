{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytest\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from email.parser import BytesParser\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of subcategory folder to category and binary label\n",
    "label_map = {\n",
    "    # Malicious\n",
    "    \"CEO_Fraud_-_Gift_Cards\": (\"gift_cards\", \"ceo_fraud\", \"malicious\"),\n",
    "    \"CEO_Fraud_-_Payroll_Update\": (\"payroll_update\", \"ceo_fraud\", \"malicious\"),\n",
    "    \"CEO_Fraud_-_Wire_Transfers\": (\"wire_transfers\", \"ceo_fraud\", \"malicious\"),\n",
    "    \"Phishing_-_3rd_Party\": (\"third_party\", \"phishing\", \"malicious\"),\n",
    "    \"Phishing_-_Outbound\": (\"outbound\", \"phishing\", \"malicious\"),\n",
    "    \"Phishing_–_UBC\": (\"ubc\", \"phishing\", \"malicious\"),\n",
    "    \"Phishing_UBC_-_Outbound\": (\"ubc_outbound\", \"phishing\", \"malicious\"),\n",
    "    \"Self-Phishing\": (\"self_phishing\", \"phishing\", \"malicious\"),\n",
    "    \"Spearphishing\": (\"spearphishing\", \"phishing\", \"malicious\"),\n",
    "    \"Reply_Chain_Attack\": (\"reply-chain-attack\", \"reply-chain-attack\", \"malicious\"),\n",
    "\n",
    "    # Benign\n",
    "    \"Legitimate_Email_Confirmed\": (\"legitimate_email_confirmed\", \"legitimate\", \"benign\"),\n",
    "    \"Spam_-_False_Positives\": (\"spam_false_positive\", \"legitimate\", \"benign\"),\n",
    "    \"Spam_–_Inbound\": (\"inbound\", \"spam\", \"benign\"),\n",
    "    \"Spam_–_Outbound\": (\"outbound\", \"spam\", \"benign\"),\n",
    "}\n",
    "\n",
    "dataset_root = Path(\"/data/dataset\")\n",
    "\n",
    "# Collect all .eml file entries\n",
    "rows = []\n",
    "for subfolder, (subcategory, category, binary_label) in label_map.items():\n",
    "    eml_files = (dataset_root / subfolder).rglob(\"*.eml\")\n",
    "    for eml in eml_files:\n",
    "        rel_path = eml.relative_to(\"/\") \n",
    "        rows.append({\n",
    "            \"path\": f\"/{rel_path.as_posix()}\",\n",
    "            \"target_1\": binary_label,\n",
    "            \"target_2\": category,\n",
    "            \"target_3\": subcategory\n",
    "        })\n",
    "\n",
    "# Build full DataFrame\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499cad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_small, _ = train_test_split(\n",
    "    df,\n",
    "    train_size=1000,\n",
    "    stratify=df[\"target_3\"],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc3e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sample_small[\"path\"].tolist()\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = [\n",
    "#     '/data/dataset/Phishing_-_3rd_Party/0a0e0cab473ff110072fbf12516d43c9/0_message.eml',\n",
    "#     '/data/dataset/Phishing_–_UBC/0ad4904a1bda559024c255b62a4bcbc3/0_message.eml',\n",
    "#     '/data/dataset/CEO_Fraud_-_Wire_Transfers/3a55d04d473bc290efc6767b416d43db/0_message.eml',\n",
    "#     '/data/dataset/CEO_Fraud_-_Gift_Cards/b9ed6f671bc6155024c255b62a4bcb1b/0_message.eml',\n",
    "#     '/data/dataset/Phishing_UBC_-_Outbound/0aeaad25938502105a9f30edfaba102e/0_message.eml'\n",
    "# ]\n",
    "\n",
    "\n",
    "emails = []\n",
    "payloads = []\n",
    "text_html = []\n",
    "text_plain = []\n",
    "text_clean = []\n",
    "\n",
    "for i, path in enumerate(paths):\n",
    "    with open(path, 'rb') as fp:\n",
    "        msg = BytesParser().parse(fp)\n",
    "        emails.append(msg)\n",
    "\n",
    "    content_type = list()\n",
    "    payload = {}\n",
    "\n",
    "    for part in msg.walk():\n",
    "        payload[part.get_content_type()] = part.get_payload(decode=True)\n",
    "\n",
    "    payloads.append(payload)\n",
    "\n",
    "    text_html.append(payload['text/html'] if 'text/html' in payload.keys() else None)\n",
    "\n",
    "    text_plain.append(payload['text/plain'].decode('utf-8', errors='replace') if 'text/plain' in payload.keys() else BeautifulSoup(payload['text/html']).get_text())\n",
    "\n",
    "    text_clean.append(' '.join(text_plain[i].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc4e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame({\n",
    "    'path': paths,\n",
    "    'email': emails,\n",
    "    'payload': payloads,\n",
    "    'text_html': text_html,\n",
    "    'text_plain': text_plain,\n",
    "    'text_clean': text_clean,\n",
    "}).set_index('path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74360e42",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef37b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def clean_text(text, min_token_len=2, irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\"]):\n",
    "    \"\"\"\n",
    "    Clean a single text string using spaCy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to be cleaned.\n",
    "    min_token_len : int, optional\n",
    "        Minimum token length to retain in the output (default is 2).\n",
    "    irrelevant_pos : list of str, optional\n",
    "        List of POS tags to ignore during filtering (default excludes function words and punctuation).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The cleaned, lemmatized, and filtered text string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove Caution tagging\n",
    "    text = text.replace(\"[CAUTION: Non-UBC Email]\", \"\")\n",
    "    \n",
    "    # Remove timestamps\n",
    "    text = re.sub(r\"\\b\\d{1,2}:\\d{2}(?:\\s*[–-]\\s*\\d{1,2}:\\d{2})?\\b\", \" \", text)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_.lower()\n",
    "\n",
    "        if (\n",
    "            not token.is_stop\n",
    "            and len(token) > min_token_len\n",
    "            and token.pos_ not in irrelevant_pos\n",
    "            and not token.is_space\n",
    "            and not token.like_email\n",
    "            and not token.like_url\n",
    "            and not token.like_num\n",
    "            and not token.is_oov\n",
    "            and not token.is_punct\n",
    "            and not token.is_digit\n",
    "            and token.ent_type_ != \"PERSON\"\n",
    "            and not re.match(r\"^\\d+(px|em|%)?$\", lemma)  # remove '10', '0px', '100%' etc.\n",
    "        ):\n",
    "            tokens.append(lemma)\n",
    "            \n",
    "    return \" \".join(tokens).strip()\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(input_data, min_token_len=2, irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\"]):\n",
    "    \"\"\"\n",
    "    Preprocess a string or a Pandas Series of strings using spaCy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : str or pandas.Series\n",
    "        A single text string or a Series of text strings to preprocess.\n",
    "    min_token_len : int, optional\n",
    "        Minimum token length to retain (default is 2).\n",
    "    irrelevant_pos : list of str, optional\n",
    "        List of POS tags to ignore (default removes common function words and punctuation).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or pandas.Series\n",
    "        Cleaned string if input is a single text, or Series of cleaned strings if input is a Series.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    TypeError\n",
    "        If input_data is neither a string nor a Pandas Series.\n",
    "    \"\"\"\n",
    "    if isinstance(input_data, pd.Series):\n",
    "        return input_data.apply(lambda text: clean_text(text, min_token_len, irrelevant_pos))\n",
    "    elif isinstance(input_data, str):\n",
    "        return clean_text(input_data, min_token_len, irrelevant_pos)\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a string or a pandas Series of strings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1fe290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_spacy(\n",
    "    input_data,\n",
    "    min_token_len=2,\n",
    "    irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess either a single text string or a Pandas Series of texts using spaCy.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_data: str or pd.Series\n",
    "    - min_token_len: Minimum token length\n",
    "    - irrelevant_pos: List of POS tags to ignore\n",
    "    \n",
    "    Returns:\n",
    "    - str (if input was a single string) or pd.Series (if input was a Series)\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = text.replace(\"[CAUTION: Non-UBC Email]\", \"\")\n",
    "        doc = nlp(text)\n",
    "\n",
    "        tokens = [\n",
    "            token.lemma_.lower()\n",
    "            for token in doc\n",
    "            if (\n",
    "                not token.is_stop\n",
    "                and len(token) > min_token_len\n",
    "                and token.pos_ not in irrelevant_pos\n",
    "                and not token.is_space\n",
    "                and not token.like_email\n",
    "                and not token.like_url\n",
    "                and not token.like_num\n",
    "                and not token.is_oov\n",
    "                and not token.is_punct\n",
    "                and not token.is_digit\n",
    "                and token.ent_type_ != \"PERSON\"\n",
    "            )\n",
    "        ]\n",
    "        return \" \".join(tokens).strip()\n",
    "\n",
    "    if isinstance(input_data, pd.Series):\n",
    "        return input_data.apply(clean_text)\n",
    "    elif isinstance(input_data, str):\n",
    "        return clean_text(input_data)\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a string or a pandas Series of strings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text_series, top_n = 5):\n",
    "    \"\"\"\n",
    "    Extract top N TF-IDF keywords from each document in a text series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_series : pandas.Series\n",
    "        Series of preprocessed text documents.\n",
    "    top_n : int\n",
    "        Number of top keywords to extract per document.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Series of lists, each containing top N keywords for a document.\n",
    "    \"\"\"\n",
    "    # Vectorize using TF-IDF\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X_tfidf = tfidf.fit_transform(text_series)\n",
    "    feature_names = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "    # For each row/document in the TF-IDF matrix, extract top N keywords\n",
    "    top_keywords_list = []\n",
    "    for row in X_tfidf:\n",
    "        row_array = row.toarray().flatten()\n",
    "        top_indices = row_array.argsort()[-top_n:][::-1]\n",
    "        keywords = feature_names[top_indices]\n",
    "        top_keywords_list.append(list(keywords))\n",
    "\n",
    "    return pd.Series(top_keywords_list, index=text_series.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_text = preprocess_spacy(data_df.text_clean)\n",
    "pp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fcd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = extract_keywords(preprocess_text(data_df.text_clean))\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfa353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kw = data_df[['text_clean']].copy()\n",
    "df_kw['keywords'] = keywords\n",
    "df_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def generate_bow(keyword):\n",
    "    \"\"\"\n",
    "    Generate a bag-of-words (BoW) representation for a given pandas Series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    keyword : pandas.Series\n",
    "        A pandas Series containing text data (strings or lists of keywords).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        A Series where each row is a dictionary representing the BoW of the input text.\n",
    "    \"\"\"\n",
    "    cv = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "    # Convert list to string if needed\n",
    "    keywords_as_strings = keyword.apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    # Fit and transform\n",
    "    bow_matrix = cv.fit_transform(keywords_as_strings)\n",
    "\n",
    "    # Feature names (vocabulary)\n",
    "    feature_names = cv.get_feature_names_out()\n",
    "\n",
    "    # Convert each row to a dictionary\n",
    "    bow_series = pd.Series([\n",
    "        dict(zip(feature_names, row.toarray().flatten()))\n",
    "        for row in bow_matrix\n",
    "    ], index=keyword.index)\n",
    "\n",
    "    return bow_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b01406",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_bow = generate_bow(extract_keywords(preprocess_text(data_df.text_clean)))\n",
    "generated_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe0561",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a2f8d",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "`conda install conda-forge::sentence-transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_df[['text_clean']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5cb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c787839",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sents = embedder.encode(data_df[\"text_clean\"].tolist())\n",
    "emb_sent_df = pd.DataFrame(emb_sents, index=data_df.index)\n",
    "emb_sent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288419f6",
   "metadata": {},
   "source": [
    "### Method 1: DBSCAN\n",
    "- No need to specify number of clusters, let it search how many clusters present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7abd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in np.arange(0.5, 0.7, 0.01):\n",
    "    print(\"\\neps={}\".format(eps))\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=2, metric=\"cosine\")\n",
    "    labels = dbscan.fit_predict(emb_sents)\n",
    "    print(\"Number of clusters: {}\".format(len(np.unique(labels))))\n",
    "    print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))\n",
    "    print(\"Cluster memberships:{}\".format(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec995425",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.64, min_samples=3, metric=\"cosine\")\n",
    "df[\"dbscan\"] = dbscan.fit_predict(emb_sents)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4f6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, colorConverter, LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['xkcd:azure', 'yellowgreen', 'tomato', 'teal', 'indigo', 'aqua', 'orangered', 'orchid', 'black', 'xkcd:turquoise', 'xkcd:violet', 'aquamarine', 'chocolate', 'darkgreen', 'sienna', 'pink', 'lightblue', 'yellow', 'lavender', 'wheat', 'linen']\n",
    "\n",
    "\n",
    "def discrete_scatter(x1, x2, y=None, markers=None, s=8, ax=None,\n",
    "                     labels=None, padding=.2, alpha=1, c=None, markeredgewidth=0.6, \n",
    "                     label_points=False, x1_annot=-0.1, x2_annot=0.2):\n",
    "    \"\"\"Adaption of matplotlib.pyplot.scatter to plot classes or clusters.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x1 : nd-array\n",
    "        input data, first axis\n",
    "    x2 : nd-array\n",
    "        input data, second axis\n",
    "    y : nd-array\n",
    "        input data, discrete labels\n",
    "    cmap : colormap\n",
    "        Colormap to use.\n",
    "    markers : list of string\n",
    "        List of markers to use, or None (which defaults to 'o').\n",
    "    s : int or float\n",
    "        Size of the marker\n",
    "    padding : float\n",
    "        Fraction of the dataset range to use for padding the axes.\n",
    "    alpha : float\n",
    "        Alpha value for all points.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if y is None:\n",
    "        y = np.zeros(len(x1))        \n",
    "\n",
    "    # unique_y = np.unique(y)\n",
    "    unique_y, inds = np.unique(y, return_index=True)    \n",
    "\n",
    "    if markers is None:\n",
    "        markers = ['o', '^', 'v', 'D', 's', '*', 'p', 'h', 'H', '8', '<', '>'] * 10\n",
    "\n",
    "    if len(markers) == 1:\n",
    "        markers = markers * len(unique_y)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = unique_y\n",
    "\n",
    "    # lines in the matplotlib sense, not actual lines\n",
    "    lines = []\n",
    "\n",
    "\n",
    "    if len(unique_y) == 1: \n",
    "        cr = [-1]\n",
    "    else: \n",
    "        cr = sorted([y[index] for index in sorted(inds)])\n",
    "\n",
    "    if c is not None and len(c) == 1: \n",
    "        cr = c\n",
    "    \n",
    "    for (i, (yy, color_ind)) in enumerate(zip(unique_y, cr)):\n",
    "        mask = y == yy\n",
    "        # print(f'color_ind= {color_ind} and i = {i}')\n",
    "        # if c is none, use color cycle\n",
    "        color = colors[color_ind]\n",
    "        # print('color: ', color)\n",
    "        # use light edge for dark markers\n",
    "        if np.mean(colorConverter.to_rgb(color)) < .2:\n",
    "            markeredgecolor = \"grey\"\n",
    "        else:\n",
    "            markeredgecolor = \"black\"\n",
    "\n",
    "        lines.append(ax.plot(x1[mask], x2[mask], markers[i], markersize=s,\n",
    "                             label=labels[i], alpha=alpha, c=color,                             \n",
    "                             markeredgewidth=markeredgewidth,\n",
    "                             markeredgecolor=markeredgecolor)[0])\n",
    "    if label_points: \n",
    "        labs = [str(label) for label in list(range(0,len(x1)))]\n",
    "        for i, txt in enumerate(labs):\n",
    "            font_size=9\n",
    "            ax.annotate(txt, (x1[i]+0.2, x2[i]+0.2), xytext= (x1[i]+x1_annot, x2[i]+x2_annot), c='k', size = font_size)\n",
    "\n",
    "    return lines    \n",
    "    \n",
    "def plot_original_clustered(X, model, labels):\n",
    "    k = np.unique(labels).shape[0]\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))    \n",
    "    ax[0].set_title(\"Original dataset\")\n",
    "    ax[0].set_xlabel(\"Feature 0\")\n",
    "    ax[0].set_ylabel(\"Feature 1\")    \n",
    "    discrete_scatter(X[:, 0], X[:, 1], ax=ax[0]);\n",
    "    # cluster the data into three clusters\n",
    "    # plot the cluster assignments and cluster centers\n",
    "    ax[1].set_title(f\"{type(model).__name__} clusters\")    \n",
    "    ax[1].set_xlabel(\"Feature 0\")\n",
    "    ax[1].set_ylabel(\"Feature 1\")\n",
    "\n",
    "    discrete_scatter(X[:, 0], X[:, 1], labels, c=labels, markers='o', ax=ax[1]); \n",
    "    if type(model).__name__ == \"KMeans\": \n",
    "        discrete_scatter(\n",
    "            model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], y=np.arange(0,k), s=15, \n",
    "            markers='*', markeredgewidth=1.0, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c003cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_original_clustered(emb_sents, dbscan, dbscan.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb47eb9",
   "metadata": {},
   "source": [
    "### Method 2: KMeans\n",
    "- Need to specify number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682484a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_emb_labels = KMeans(n_clusters=12, n_init='auto', random_state=42)\n",
    "kmeans_emb_labels.fit(emb_sent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"emb_kmeans\"] = kmeans_emb_labels.labels_\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479fa233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "model = KMeans(n_init='auto')\n",
    "visualizer = KElbowVisualizer(model, k=(1, 20))\n",
    "\n",
    "visualizer.fit(emb_sents)  # Fit the data to the visualizer\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a5b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "model = KMeans(12, n_init='auto', random_state=42)\n",
    "visualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\n",
    "visualizer.fit(emb_sents)  # Fit the data to the visualizer\n",
    "visualizer.show();\n",
    "# Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b58bc6",
   "metadata": {},
   "source": [
    "### Method 3: BERTopic\n",
    "- No need to specify number of clusters, it can generate the topic keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171aa1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c372df",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data_df['text_clean']\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aed6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# Fine-tune your topic representations\n",
    "representation_model = KeyBERTInspired()\n",
    "topic_model = BERTopic(representation_model=representation_model)\n",
    "\n",
    "#topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f301cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac095e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adfbd4a",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "`conda install conda-forge::spacy`\n",
    "\n",
    "`python -m spacy download en_core_web_md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480fe45c",
   "metadata": {},
   "source": [
    " Data cleaning with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb811aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_spacy(\n",
    "    doc,\n",
    "    min_token_len=2,\n",
    "    irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text\n",
    "    and return a preprocessed string.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    doc : (spaCy doc object)\n",
    "        the spacy doc object of the text\n",
    "    min_token_len : (int)\n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list)\n",
    "        a list of irrelevant pos tags\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "    # Remove specific caution text from the 'text_clean' column if it exists\n",
    "    caution_text = \"[CAUTION: Non-UBC Email]\"\n",
    "    if caution_text in doc.text:\n",
    "        doc = nlp(doc.text.replace(caution_text, \"\"))\n",
    "\n",
    "    clean_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        if (\n",
    "            token.is_stop == False  # Check if it's not a stopword\n",
    "            and len(token) > min_token_len  # Check if the word meets minimum threshold\n",
    "            and token.pos_ not in irrelevant_pos\n",
    "            and token.is_space == False\n",
    "            and token.like_email == False\n",
    "            and token.like_url == False\n",
    "            and token.like_num == False\n",
    "            and token.is_oov == False\n",
    "            and token.is_punct == False\n",
    "            and token.is_digit == False\n",
    "            and token.ent_type_ != \"PERSON\"  # Exclude tokens identified as names\n",
    "            \n",
    "        ):  # Check if the POS is in the acceptable POS tags\n",
    "            lemma = token.lemma_  # Take the lemma of the word\n",
    "            clean_text.append(lemma.lower())\n",
    "    return \" \".join(clean_text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_df[['text_clean']].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf0bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_pp\"] = [preprocess_spacy(text) for text in nlp.pipe(df[\"text_clean\"])]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1398120",
   "metadata": {},
   "source": [
    "LDA Model\n",
    "\n",
    "`pip install scipy gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f554ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "corpus = [doc.split() for doc in df[\"text_pp\"].tolist()]\n",
    "dictionary = Dictionary(corpus)  # Create a vocabulary for the lda model\n",
    "#dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "K = [6,8,10,12,14]\n",
    "\n",
    "coherence_scores = []\n",
    "\n",
    "for num_topics in K:\n",
    "    lda = LdaModel(\n",
    "        corpus=doc_term_matrix,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        random_state=42,\n",
    "        passes=10,\n",
    "    )\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        model=lda, texts=corpus, dictionary=dictionary, coherence=\"c_v\"\n",
    "    )\n",
    "    coherence_scores.append(coherence_model_lda.get_coherence())\n",
    "\n",
    "cs_df = pd.DataFrame(coherence_scores, index=K, columns=[\"Coherence score\"])\n",
    "cs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_df.plot(title=\"Coherence scores\", xlabel=\"num_topics\", ylabel=\"Coherence score\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "lda = LdaModel(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    ")\n",
    "\n",
    "topics = lda.print_topics(num_topics = 12, num_words=5,)  \n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20be70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topic(0, topn=5)  # Topic 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9178dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"topic\"] = [lda.get_document_topics(bow) for bow in doc_term_matrix]\n",
    "# df[\"topic_keywords\"] = [\", \".join([word for word, _ in lda.show_topic(max(doc, key=lambda x: x[1])[0], topn=15)]) for doc in df[\"topic\"]]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"topic\"] = [max(lda.get_document_topics(bow), key=lambda x: x[1]) if lda.get_document_topics(bow) else (None, 0) for bow in doc_term_matrix]\n",
    "df[\"topic_keywords\"] = [\n",
    "\t\", \".join([word for word, _ in lda.show_topic(topic[0], topn=15)]) if topic[0] is not None else \"\" \n",
    "\tfor topic in df[\"topic\"]\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae6784",
   "metadata": {},
   "source": [
    "### Topic2Vec modelling\n",
    "`conda install conda-forge::top2vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5195ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d538869",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Top2Vec(documents,embedding_model='distiluse-base-multilingual-cased', min_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f16d0",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_in_chunks(texts, chunk_size=512, merge_fn=None):\n",
    "    \"\"\"\n",
    "    Process a list of texts by splitting them into chunks and merging the results.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: list of str, the input texts to process.\n",
    "    - chunk_size: int, the maximum size of each chunk.\n",
    "    - merge_fn: callable, a function to merge the processed chunks (e.g., averaging scores).\n",
    "\n",
    "    Returns:\n",
    "    - list, the merged results for each text.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        # Split the text into chunks\n",
    "        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        \n",
    "        # Process each chunk\n",
    "        chunk_results = sentiment_pipeline(chunks)\n",
    "        \n",
    "        # Merge the results using the provided merge function\n",
    "        if merge_fn:\n",
    "            merged_result = merge_fn(chunk_results)\n",
    "        else:\n",
    "            merged_result = chunk_results  # Default: no merging\n",
    "        \n",
    "        results.append(merged_result)\n",
    "    return results\n",
    "\n",
    "# Example merge function: averaging sentiment scores\n",
    "def merge_sentiment_scores(chunk_results):\n",
    "    positive_scores = [res['score'] for res in chunk_results if res['label'] == 'POSITIVE']\n",
    "    negative_scores = [res['score'] for res in chunk_results if res['label'] == 'NEGATIVE']\n",
    "    return {\n",
    "        'POSITIVE': sum(positive_scores) / len(positive_scores) if positive_scores else 0,\n",
    "        'NEGATIVE': sum(negative_scores) / len(negative_scores) if negative_scores else 0,\n",
    "    }\n",
    "\n",
    "# Process the text_clean column in chunks and merge results\n",
    "chunked_sentiment_results = process_text_in_chunks(df[\"text_pp\"].tolist(), chunk_size=512, merge_fn=merge_sentiment_scores)\n",
    "chunked_sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d635dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"] = chunked_sentiment_results\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df547d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-jiaquan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
