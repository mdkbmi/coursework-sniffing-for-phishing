{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from email.parser import BytesParser\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724066cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a000359",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    '/data/dataset/Phishing_-_3rd_Party/0a0e0cab473ff110072fbf12516d43c9/0_message.eml',\n",
    "    '/data/dataset/Phishing_–_UBC/0ad4904a1bda559024c255b62a4bcbc3/0_message.eml',\n",
    "    '/data/dataset/CEO_Fraud_-_Wire_Transfers/3a55d04d473bc290efc6767b416d43db/0_message.eml',\n",
    "    '/data/dataset/CEO_Fraud_-_Gift_Cards/b9ed6f671bc6155024c255b62a4bcb1b/0_message.eml',\n",
    "    '/data/dataset/Phishing_UBC_-_Outbound/0aeaad25938502105a9f30edfaba102e/0_message.eml'\n",
    "]\n",
    "\n",
    "emails = []\n",
    "payloads = []\n",
    "text_html = []\n",
    "text_plain = []\n",
    "text_clean = []\n",
    "\n",
    "for i, path in enumerate(paths):\n",
    "    with open(path, 'rb') as fp:\n",
    "        msg = BytesParser().parse(fp)\n",
    "        emails.append(msg)\n",
    "\n",
    "    content_type = list()\n",
    "    payload = {}\n",
    "\n",
    "    for part in msg.walk():\n",
    "        payload[part.get_content_type()] = part.get_payload(decode=True)\n",
    "\n",
    "    payloads.append(payload)\n",
    "\n",
    "    text_html.append(payload['text/html'] if 'text/html' in payload.keys() else None)\n",
    "\n",
    "    text_plain.append(payload['text/plain'].decode() if 'text/plain' in payload.keys() else BeautifulSoup(payload['text/html']).get_text())\n",
    "\n",
    "    text_clean.append(' '.join(text_plain[i].split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_old = pd.DataFrame({\n",
    "    'path': paths,\n",
    "    'email': emails,\n",
    "    'payload': payloads,\n",
    "    'text_html': text_html,\n",
    "    'text_plain': text_plain,\n",
    "    'text_clean': text_clean,\n",
    "}).set_index('path')\n",
    "\n",
    "\n",
    "data_df_old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c1309c",
   "metadata": {},
   "source": [
    "# filter eml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dirs = [\n",
    "    \"CEO_Fraud_-_Gift_Cards\",\n",
    "    \"CEO_Fraud_-_Payroll_Update\",\n",
    "    \"CEO_Fraud_-_Wire_Transfers\",\n",
    "    \"Legitimate_Email_Confirmed\",\n",
    "    \"Phishing_-_3rd_Party\",\n",
    "    \"Phishing_-_Outbound\",\n",
    "    \"Phishing_–_UBC\",\n",
    "    \"Phishing_UBC_-_Outbound\",\n",
    "    \"Reply_Chain_Attack\",\n",
    "    \"Self-Phishing\",\n",
    "    \"Spam_-_False_Positives\",\n",
    "    \"Spam_–_Inbound\",\n",
    "    \"Spam_–_Outbound\",\n",
    "    \"Spearphishing\"\n",
    "]\n",
    "\n",
    "root_dir = Path(\"/data/dataset\")\n",
    "\n",
    "eml_files = []\n",
    "for folder in target_dirs:\n",
    "    folder_path = root_dir / folder\n",
    "    if folder_path.exists():\n",
    "        for eml in folder_path.rglob(\"*_message.eml\"):\n",
    "            eml_files.append((folder, str(eml)))\n",
    "\n",
    "df_eml = pd.DataFrame(eml_files, columns=[\"label\", \"path\"])\n",
    "\n",
    "df_eml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d8604",
   "metadata": {},
   "source": [
    "# sample 30 from each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df_eml.groupby(\"label\").apply(lambda x: x.sample(n=min(30, len(x)), random_state=42)).reset_index(drop=True)\n",
    "\n",
    "emails = []\n",
    "payloads = []\n",
    "text_html = []\n",
    "text_plain = []\n",
    "text_clean = []\n",
    "\n",
    "for i, path in enumerate(sample_df[\"path\"]):\n",
    "    try:\n",
    "        with open(path, 'rb') as fp:\n",
    "            msg = BytesParser().parse(fp)\n",
    "            emails.append(msg)\n",
    "\n",
    "            payload = {}\n",
    "            for part in msg.walk():\n",
    "                ctype = part.get_content_type()\n",
    "                try:\n",
    "                    payload[ctype] = part.get_payload(decode=True)\n",
    "                except:\n",
    "                    payload[ctype] = None\n",
    "            payloads.append(payload)\n",
    "\n",
    "            html = payload.get(\"text/html\", None)\n",
    "            plain = payload.get(\"text/plain\", None)\n",
    "\n",
    "            if html and not plain:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(html, \"html.parser\")\n",
    "                    plain = soup.get_text().encode()\n",
    "                except:\n",
    "                    plain = b\"\"\n",
    "\n",
    "            try:\n",
    "                decoded_plain = plain.decode() if plain else \"\"\n",
    "            except:\n",
    "                decoded_plain = \"\"\n",
    "\n",
    "            text_html.append(html)\n",
    "            text_plain.append(decoded_plain)\n",
    "            text_clean.append(\" \".join(decoded_plain.split()))\n",
    "    except Exception as e:\n",
    "        emails.append(None)\n",
    "        payloads.append({})\n",
    "        text_html.append(None)\n",
    "        text_plain.append(\"\")\n",
    "        text_clean.append(\"\")\n",
    "\n",
    "data_df = pd.DataFrame({\n",
    "    \"label\": sample_df[\"label\"].values,\n",
    "    \"path\": sample_df[\"path\"].values,\n",
    "    \"email\": emails,\n",
    "    \"payload\": payloads,\n",
    "    \"text_html\": text_html,\n",
    "    \"text_plain\": text_plain,\n",
    "    \"text_clean\": text_clean\n",
    "}).set_index(\"path\")\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8aa92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eml[\"label\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f66bb1",
   "metadata": {},
   "source": [
    "# Extract URLs from .eml File\n",
    "*urlextract package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(text=None, html=None):\n",
    "    \"\"\"\n",
    "    Extract URLs from either plain text or HTML.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str or pd.Series, optional\n",
    "        Plain text (cleaned).\n",
    "    html : str or pd.Series, optional\n",
    "        Raw HTML content.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list or pd.Series\n",
    "        List of extracted URLs from both text and HTML.\n",
    "    \"\"\"\n",
    "\n",
    "    url_pattern = re.compile(r'(https?://[^\\s\\)\\]\\<\\>\\\"\\'\\,]+|www\\.[^\\s\\)\\]\\<\\>\\\"\\'\\,]+)')\n",
    "\n",
    "    def _extract_combined(txt, html_str):\n",
    "        urls = []\n",
    "        \n",
    "        if isinstance(txt, str):\n",
    "            matches = url_pattern.findall(txt)\n",
    "            urls.extend(u if u.startswith(('http://', 'https://')) else 'http://' + u for u in matches)\n",
    "        \n",
    "        if isinstance(html_str, str):\n",
    "            try:\n",
    "                soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "                urls.extend(a.get(\"href\") for a in soup.find_all(\"a\") if a.get(\"href\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return list(set(urls)) \n",
    "\n",
    "    \n",
    "    if isinstance(text, pd.Series) or isinstance(html, pd.Series):\n",
    "        return pd.Series([\n",
    "            _extract_combined(t, h)\n",
    "            for t, h in zip(\n",
    "                text if isinstance(text, pd.Series) else [None]*len(html),\n",
    "                html if isinstance(html, pd.Series) else [None]*len(text)\n",
    "            )\n",
    "        ])\n",
    "    else:\n",
    "        return _extract_combined(text, html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_lists = extract_urls(data_df.text_clean)\n",
    "url_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4feb20",
   "metadata": {},
   "source": [
    "# Count the number of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_count(text):\n",
    "    \"\"\"\n",
    "    Count the number of URLs in a given text or Series of texts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str or pd.Series\n",
    "        Raw or cleaned email text(s). Can be a single string or a pandas Series.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int or pd.Series\n",
    "        Number of URLs in the input text(s).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> get_url_count(\"Visit https://ubc.ca or http://example.com\")\n",
    "    2\n",
    "\n",
    "    >>> pd.Series([\"https://a.com\", \"no link\"]).pipe(get_url_count)\n",
    "    0    1\n",
    "    1    0\n",
    "    dtype: int64\n",
    "    \"\"\"\n",
    "    urls = extract_urls(text)\n",
    "    if isinstance(urls, pd.Series):\n",
    "        return urls.apply(len)\n",
    "    else:\n",
    "        return len(urls)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b3cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_count = get_url_count(data_df.text_clean)\n",
    "\n",
    "url_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56deaeb",
   "metadata": {},
   "source": [
    "# Check if URLs accessible (Ture if there exist at least 1 accessible URL for saving source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f950ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_accessible_url(urls, timeout=3):\n",
    "    \"\"\"\n",
    "    Check whether at least one URL in a list is accessible (i.e., returns HTTP 200).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls : list of str or str\n",
    "        A list of URL strings (or a single URL string) to check.\n",
    "    timeout : int, optional (default=3)\n",
    "        Timeout in seconds for each request.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if at least one URL is accessible; False if all are unreachable or invalid.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> has_accessible_url([\"https://ubc.ca\", \"http://invalid.domain\"])\n",
    "    True\n",
    "    \"\"\"\n",
    "    if isinstance(urls, str):\n",
    "        urls = [urls]\n",
    "    if not isinstance(urls, list):\n",
    "        return False\n",
    "\n",
    "    for url in urls:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = \"http://\" + url\n",
    "        try:\n",
    "            resp = requests.head(url, allow_redirects=True, timeout=timeout)\n",
    "            if resp.status_code == 200:\n",
    "                return True\n",
    "        except requests.RequestException:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca094aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accessible = url_lists.apply(has_accessible_url)\n",
    "check_accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accessible[check_accessible]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e4b36",
   "metadata": {},
   "source": [
    "# Check if redirection happened (True if redirection happened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f487e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_redirected_url(urls, timeout=3):\n",
    "    \"\"\"\n",
    "    Check whether any URL in the list performs a redirection to a different destination.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls : list of str or str\n",
    "        A list of URLs or a single URL string.\n",
    "    timeout : int, default 3\n",
    "        Timeout for each request in seconds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if any URL redirects to a different destination;\n",
    "        False if none redirect or all fail to connect.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize to list\n",
    "    if isinstance(urls, str):\n",
    "        urls = [urls]\n",
    "    if not isinstance(urls, list):\n",
    "        return False\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    def normalize(url):\n",
    "        parsed = urlparse(url)\n",
    "        return parsed.geturl().rstrip('/').lower()\n",
    "\n",
    "    for url in urls:\n",
    "        if not isinstance(url, str) or not url.strip():\n",
    "            continue\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = \"http://\" + url\n",
    "\n",
    "        try:\n",
    "            # First try HEAD\n",
    "            response = requests.head(url, allow_redirects=True, timeout=timeout, headers=headers)\n",
    "            final_url = response.url\n",
    "        except requests.RequestException:\n",
    "            try:\n",
    "                # Fallback to GET\n",
    "                response = requests.get(url, allow_redirects=True, timeout=timeout, headers=headers, stream=True)\n",
    "                final_url = response.url\n",
    "            except requests.RequestException:\n",
    "                continue\n",
    "\n",
    "        if normalize(final_url) != normalize(url):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66393835",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_redirection = url_lists.apply(has_redirected_url)\n",
    "check_redirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c32e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_redirection[check_redirection]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d470a576",
   "metadata": {},
   "source": [
    "# Check if URLs contain IP address  eg.http://111.123.1.1\n",
    "- hide real domain\n",
    "- Rapid deployment\n",
    "- avoid blacklist filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b77344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def has_ip_url(urls):\n",
    "    \"\"\"\n",
    "    Check whether any of the given URLs contains an IP address.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls : str or list of str\n",
    "        A single URL string or a list of URL strings to be checked.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if any URL in the input contains an IP address.\n",
    "        Returns False for invalid input types or if no IP address is found.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function treats any non-string elements in a list as invalid and skips them.\n",
    "    - If a URL doesn't start with 'http://' or 'https://', 'http://' is prepended before checking.\n",
    "    - A valid IP address pattern matches 'http(s)://<digit>.<digit>.<digit>.<digit>' at the start.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> has_ip_url(\"http://192.168.0.1/index.html\")\n",
    "    True\n",
    "\n",
    "    >>> has_ip_url([\"https://example.com\", \"http://172.16.0.1\"])\n",
    "    True\n",
    "\n",
    "    >>> has_ip_url([\"https://example.com\", \"not a url\"])\n",
    "    False\n",
    "\n",
    "    >>> has_ip_url(12345)\n",
    "    False\n",
    "    \"\"\"\n",
    "    ip_pattern = re.compile(r'^https?://(\\d{1,3}\\.){3}\\d{1,3}')\n",
    "\n",
    "    # Reject unsupported input types early\n",
    "    if not isinstance(urls, (list, str)):\n",
    "        return False\n",
    "\n",
    "    # Normalize to list for uniform processing\n",
    "    if isinstance(urls, str):\n",
    "        urls = [urls]\n",
    "\n",
    "    # Check each URL\n",
    "    for u in urls:\n",
    "        if not isinstance(u, str) or not u.strip():\n",
    "            continue  # Skip non-string or empty entries\n",
    "        url_to_check = u if u.startswith((\"http://\", \"https://\")) else \"http://\" + u\n",
    "        if ip_pattern.match(url_to_check):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1bd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ip = url_lists.apply(has_ip_url)\n",
    "\n",
    "check_ip[check_ip == True]\n",
    "\n",
    "check_ip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9792202",
   "metadata": {},
   "source": [
    "# Check if URLs starts with http\n",
    "- http is old, cheap and faster for setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f215224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_http_only(urls):\n",
    "    \"\"\"\n",
    "    Check whether any of the given URLs explicitly starts with 'http://' (not 'https://').\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls : str or list of str\n",
    "        A single URL string or a list of URL strings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if any URL in the input starts with 'http://' only (not 'https://').\n",
    "        Returns False for invalid input or if no such URL is found.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> has_http_only(\"http://example.com\")\n",
    "    True\n",
    "\n",
    "    >>> has_http_only([\"https://secure.com\", \"http://open.com\"])\n",
    "    True\n",
    "\n",
    "    >>> has_http_only([\"https://secure.com\", \"www.example.com\"])\n",
    "    False\n",
    "    \"\"\"\n",
    "    if not isinstance(urls, (str, list)):\n",
    "        return False\n",
    "\n",
    "    if isinstance(urls, str):\n",
    "        urls = [urls]\n",
    "\n",
    "    for u in urls:\n",
    "        if isinstance(u, str) and u.startswith(\"http://\"):\n",
    "            return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3600ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_http = url_lists.apply(has_http_only)\n",
    "\n",
    "check_http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_http[check_http == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db9804",
   "metadata": {},
   "source": [
    "# Check if URLs has @ symbol\n",
    "- @ can hide real link, everything before @ are treated as login info. http://UBC.com@bad.com actually you will go to bad.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7fff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_at_symbol(urls):\n",
    "    \"\"\"\n",
    "    Check whether any of the given URLs contains an '@' symbol.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls : str or list of str\n",
    "        A single URL string or a list of URL strings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if any URL in the input contains an '@' symbol.\n",
    "        Returns False for invalid input or if no such symbol is found.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> has_at_symbol(\"http://user@example.com\")\n",
    "    True\n",
    "\n",
    "    >>> has_at_symbol([\"https://example.com\", \"http://admin@evil.com\"])\n",
    "    True\n",
    "\n",
    "    >>> has_at_symbol([\"https://safe.com\", \"http://normal.com\"])\n",
    "    False\n",
    "\n",
    "    >>> has_at_symbol(12345)\n",
    "    False\n",
    "    \"\"\"\n",
    "    if not isinstance(urls, (str, list)):\n",
    "        return False\n",
    "\n",
    "    if isinstance(urls, str):\n",
    "        urls = [urls]\n",
    "\n",
    "    for u in urls:\n",
    "        if isinstance(u, str) and \"@\" in u:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3aa6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_at = url_lists.apply(has_at_symbol)\n",
    "check_at[check_at == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_at = url_lists.apply(has_at_symbol)\n",
    "\n",
    "check_at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8b872",
   "metadata": {},
   "source": [
    "# Check if URLs has port number\n",
    "- Normal website won't include prot number (http:80  https:443)\n",
    "- easy setup\n",
    "- avoid firewall filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97acdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_port_number(urls):\n",
    "    \"\"\"\n",
    "    Check whether any of the given URLs includes an explicit port number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls : str or list of str\n",
    "        A single URL string or a list of URL strings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if any URL contains an explicit port number (e.g., ':8080').\n",
    "        False for invalid input or if no port is specified.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> has_port_number(\"http://example.com:8080\")\n",
    "    True\n",
    "\n",
    "    >>> has_port_number([\"https://abc.com\", \"http://site.org:8000/page\"])\n",
    "    True\n",
    "\n",
    "    >>> has_port_number([\"https://example.com\", \"http://abc.com\"])\n",
    "    False\n",
    "\n",
    "    >>> has_port_number(12345)\n",
    "    False\n",
    "    \"\"\"\n",
    "    if not isinstance(urls, (str, list)):\n",
    "        return False\n",
    "\n",
    "    if isinstance(urls, str):\n",
    "        urls = [urls]\n",
    "\n",
    "    for u in urls:\n",
    "        if not isinstance(u, str) or not u.strip():\n",
    "            continue\n",
    "        # Add scheme if missing for proper parsing\n",
    "        full_url = u if u.startswith((\"http://\", \"https://\")) else \"http://\" + u\n",
    "        parsed = urlparse(full_url)\n",
    "        if parsed.port is not None:\n",
    "            return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f816549",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_port_number = url_lists.apply(has_port_number)\n",
    "\n",
    "check_port_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_port_number[check_port_number == True]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a951f95",
   "metadata": {},
   "source": [
    "# Check if URLs too long (default threshold = 75)\n",
    "- confuse user\n",
    "- most normal link are simple and short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3164f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_long_url(urls, threshold=75):\n",
    "    \"\"\"\n",
    "    Check whether any of the given URLs exceeds a specified length threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls : str or list of str\n",
    "        A single URL string or a list of URL strings.\n",
    "    threshold : int, optional (default=75)\n",
    "        The minimum length at which a URL is considered \"long\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if any URL is longer than the threshold.\n",
    "        False for invalid input or if all URLs are within limit.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> has_long_url(\"http://short.com\")\n",
    "    False\n",
    "\n",
    "    >>> has_long_url(\"http://verylongurl.com/\" + \"a\"*80)\n",
    "    True\n",
    "\n",
    "    >>> has_long_url([\"http://a.com\", \"http://b.com/\" + \"x\"*100], threshold=90)\n",
    "    True\n",
    "\n",
    "    >>> has_long_url(None)\n",
    "    False\n",
    "    \"\"\"\n",
    "    if not isinstance(urls, (str, list)):\n",
    "        return False\n",
    "\n",
    "    if isinstance(urls, str):\n",
    "        urls = [urls]\n",
    "\n",
    "    for u in urls:\n",
    "        if isinstance(u, str) and len(u) > threshold:\n",
    "            return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b30b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_long_url = url_lists.apply(has_long_url)\n",
    "\n",
    "check_long_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c4c49",
   "metadata": {},
   "source": [
    "# Check if URLs has multiple subdomains\n",
    "- confuse user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e247d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_multiple_subdomains(urls):\n",
    "    \"\"\"\n",
    "    Check whether any of the given URLs has more than 2 dots in its domain,\n",
    "    indicating the presence of multiple subdomains (e.g., a.b.c.com).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls : str or list of str\n",
    "        A single URL string or a list of URL strings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if any URL contains more than one subdomain (i.e., more than 2 dots in domain).\n",
    "        False for invalid input or normal domain structures like 'www.example.com'.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> has_multiple_subdomains(\"http://a.b.c.com\")\n",
    "    True\n",
    "\n",
    "    >>> has_multiple_subdomains([\"example.com\", \"x.y.z.domain.com\"])\n",
    "    True\n",
    "\n",
    "    >>> has_multiple_subdomains(\"http://www.example.com\")\n",
    "    False\n",
    "\n",
    "    >>> has_multiple_subdomains(12345)\n",
    "    False\n",
    "    \"\"\"\n",
    "    if not isinstance(urls, (str, list)):\n",
    "        return False\n",
    "\n",
    "    if isinstance(urls, str):\n",
    "        urls = [urls]\n",
    "\n",
    "    for u in urls:\n",
    "        if not isinstance(u, str) or not u.strip():\n",
    "            continue\n",
    "        url_to_check = u if u.startswith((\"http://\", \"https://\")) else \"http://\" + u\n",
    "        parsed = urlparse(url_to_check)\n",
    "        domain_parts = parsed.netloc.split(\".\")\n",
    "        # e.g. 'a.b.c.com' → ['a', 'b', 'c', 'com'] → 4 parts → subdomains = 4 - 2 = 2 → True\n",
    "        if len(domain_parts) > 3:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_subdomains = url_lists.apply(has_multiple_subdomains)\n",
    "\n",
    "check_subdomains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-xuci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
