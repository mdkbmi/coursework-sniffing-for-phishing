{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c079fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    'unsloth/Phi-4-mini-reasoning-GGUF',\n",
    "    filename='Phi-4-mini-reasoning-Q4_K_M.gguf',\n",
    "    n_ctx=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03082770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353fe7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_list = pd.read_csv('/data/workspace/dataset/sampled-dataset/sample-small.csv').query('`target_3` != \"self_phishing\"')\n",
    "email_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.message import Message\n",
    "from email.parser import BytesParser\n",
    "import re\n",
    "import base64\n",
    "from email.header import decode_header, make_header\n",
    "\n",
    "def __decode_mime_header(header_value):\n",
    "    \"\"\"Decode MIME-encoded email headers to readable text\"\"\"\n",
    "    try:\n",
    "        decoded_header = decode_header(header_value)\n",
    "        return str(make_header(decoded_header))\n",
    "    except Exception as e:\n",
    "        print(f\"Header decoding error: {e}\")\n",
    "        return f\"<Unable to decode: {header_value}>\"\n",
    "    \n",
    "def __decode_email_content(contents):\n",
    "    decoded_contents = []\n",
    "\n",
    "    for each in contents:\n",
    "        for ct, c in each.items():\n",
    "            if ct in ['text/html', 'text/plain']:\n",
    "                try:\n",
    "                    c = base64.b64decode(c).decode('utf-8')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            decoded_contents.append({ct: c})\n",
    "\n",
    "    return decoded_contents\n",
    "\n",
    "\n",
    "def __open_email(p: str):\n",
    "    email = {}\n",
    "\n",
    "    with open(p, 'rb') as fp:\n",
    "        email['path'] = p\n",
    "        msg = BytesParser().parse(fp)\n",
    "    \n",
    "    header = msg.items()\n",
    "    email['header'] = {}\n",
    "\n",
    "    for key, value in header:\n",
    "        if key == 'Received':\n",
    "            if key not in email['header']:\n",
    "                email['header'][key] = []\n",
    "            \n",
    "            email['header'][key].append(value)\n",
    "        elif not key.upper().startswith('X-') and not key.lower().startswith('ironport-'):\n",
    "            email['header'][key] = value\n",
    "\n",
    "    \n",
    "    email['header']['Subject'] = __decode_mime_header(email['header']['Subject'])\n",
    "\n",
    "    contents = []\n",
    "\n",
    "    for part in msg.walk():\n",
    "        content_type = part.get_content_type()\n",
    "        content = part.get_payload()\n",
    "        contents.append({content_type: content})\n",
    "\n",
    "    email['content'] = contents\n",
    "    email['decoded_content'] = __decode_email_content(contents)\n",
    "\n",
    "    return email\n",
    "\n",
    "def open_email(path):\n",
    "    if isinstance(path, str):\n",
    "        emails = __open_email(path)\n",
    "    \n",
    "    elif isinstance(path, pd.Series):\n",
    "        emails = path.apply(__open_email).to_list()\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Path must be a string or pandas Series\")\n",
    "    \n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = pd.DataFrame(open_email(email_list.path)).set_index('path')\n",
    "targets = email_list.set_index('path')['target_1']\n",
    "emails = emails.join(targets)\n",
    "\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_benign = emails[emails['target_1'] == 'benign']\n",
    "emails_benign.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e36c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails.iterrows()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = emails_benign.iloc[1]\n",
    "header = example.header\n",
    "content = example.decoded_content\n",
    "label = example.target_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a138f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b253e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_feature_extraction(label: str) -> str:\n",
    "    if label in ['benign']:\n",
    "        true_label = 'benign'\n",
    "        opp_label = 'malicious'\n",
    "    else:\n",
    "        true_label = 'malicious'\n",
    "        opp_label = 'benign'\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a cybersecurity analyst at the University of British Columbia (UBC) in Canada and you are an expert in email security. You are building a machine learning model to classify emails reported as suspicious. Your colleague has labeled this email as {true_label}, and you are analyzing what features of the email are associated with that label.\n",
    "\n",
    "Label definitions:\n",
    "1. 'benign': Emails that do not pose urgent harm to the recipient. This includes legitimate emails, emails from legitimate senders, and spam that appears suspicious but does not contain malicious links or attachments. These include social engineering attempts that do not contain malicious links or attachments.\n",
    "2. 'malicious': Emails that can compromise sensitive information or cause financial distress, including phishing, CEO fraud, and reply chain attacks. These often contain malicious links or malware.\n",
    "\n",
    "Analyze the provided email header and/or content as follows:\n",
    "1. Provide exactly three distinct reasons in favor of the email being labeled as '{true_label}', referencing the specific part of the email (quote or summarize relevant section).\n",
    "2. Provide exactly two strong reasons why the email is unlikely to be '{opp_label}', referencing the email as above.\n",
    "3. Provide exactly one plausible reason why the email could be '{opp_label}' instead of '{true_label}'.\n",
    "\n",
    "IMPORTANT: In your analysis, IGNORE any [CAUTION: Non-UBC Email] labels.\n",
    "\n",
    "Format your response as a numbered list under each step. If evidence is insufficient, explain your reasoning and indicate any uncertainties.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd55e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_header = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_feature_extraction(label)}, \n",
    "        {\"role\": \"user\", \"content\": f'{header}'}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d009fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_header['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a3e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_response(response_text):\n",
    "    \"\"\"Remove the thinking process from LLM responses\"\"\"\n",
    "    import re\n",
    "    # Remove content between <think> tags\n",
    "    cleaned = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL)\n",
    "    # Remove any empty lines that might remain\n",
    "    cleaned = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "# Use when printing the output\n",
    "print(clean_llm_response(evaluation_header['choices'][0]['message']['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb93ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_content = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_feature_extraction(label)}, \n",
    "        {\"role\": \"user\", \"content\": f'From: {header['From']}, To: {header['To']}, Subject: {header['Subject']}, {content}'}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb0a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_content['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6382835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_llm_response(evaluation_content['choices'][0]['message']['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_prompt = \"Based on this analysis, create ONLY a JSON response in this exact format: {\\\"label\\\": \\\"benign\\\" or \\\"malicious\\\", \\\"confidence_level\\\": \\\"not confident\\\" or \\\"somewhat confident\\\" or \\\"confident\\\" or \\\"extremely confident\\\", \\\"justification\\\": [\\\"characteristic1\\\", \\\"characteristic2\\\", \\\"characteristic3\\\"]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = {\n",
    "    \"type\": \"json_object\",\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"label\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"benign\", \"malicious\"]\n",
    "            },\n",
    "            \"confidence_level\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"not confident\", \"somewhat confident\", \"confident\", \"extremely confident\"]\n",
    "            },\n",
    "            \"justification\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"minItems\": 3,\n",
    "                \"maxItems\": 3\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"label\", \"confidence_level\", \"justification\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_response = summary_llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": summarise_prompt},\n",
    "        {\"role\": \"user\", \"content\": f'HEADER: {evaluation_header['choices'][0]['message']['content']}\\n\\nCONTENT: {content_evaluation['choices'][0]['message']['content']}'}\n",
    "    ],\n",
    "    response_format=response_format,\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb81cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23279cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982decf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get the string response\n",
    "response_text = structured_response['choices'][0]['message']['content']\n",
    "\n",
    "# Convert string to JSON object\n",
    "email_analysis = json.loads(response_text)\n",
    "\n",
    "# Now you can access individual fields\n",
    "print(f\"Email classification: {email_analysis['label']}\")\n",
    "print(f\"Confidence level: {email_analysis['confidence_level']}\")\n",
    "print(\"\\nJustifications:\")\n",
    "for i, reason in enumerate(email_analysis['justification'], 1):\n",
    "    print(f\"{i}. {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9390cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.read_parquet('/data/workspace/danishki/git_repo/notebooks/milestone4/llm_results_checkpoint_10.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729210a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe816a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-danishki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
